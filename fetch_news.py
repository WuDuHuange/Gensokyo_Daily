#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¹»æƒ³ä¹¡æ—¥æŠ¥ (Gensokyo Daily) â€” æ–°é—»æŠ“å–è„šæœ¬
ä½¿ç”¨ RSSHub ä½œä¸ºæ•°æ®ä¸­é—´ä»¶ï¼Œèšåˆä¸œæ–¹ Project ç›¸å…³èµ„è®¯ã€‚
"""

import json
import os
import re
import time
import hashlib
import uuid
import random
import functools
from datetime import datetime, timedelta, timezone
from typing import Optional
from urllib.parse import urlencode

import feedparser
import requests

# ============================================================
# âš™ï¸ Bç«™åˆ†åŒºé…ç½® (ID ä¸å˜)
# ============================================================
BILIBILI_PARTITIONS = [
    {"name": "Bç«™ MMDæ¦œ", "rid": 25, "icon": "ğŸ’ƒ", "priority": 1},
    {"name": "Bç«™ æ‰‹ä¹¦æ¦œ", "rid": 24, "icon": "ğŸ¬", "priority": 1},
    {"name": "Bç«™ éŸ³ä¹æ¦œ", "rid": 28, "icon": "ğŸµ", "priority": 2},
    {"name": "Bç«™ æ¸¸æˆæ¦œ", "rid": 17, "icon": "ğŸ®", "priority": 2},
]

# ============================================================
# Bç«™ WBI ç­¾åé­”æ³• (Copy & Paste)
# ============================================================
def get_mixin_key(orig: str):
    'å¯¹ imgKey å’Œ subKey è¿›è¡Œå­—ç¬¦é¡ºåºæ‰“ä¹±ç¼–ç '
    mixin_key_enc_tab = [
        46, 47, 18, 2, 53, 8, 23, 32, 15, 50, 10, 31, 58, 3, 45, 35, 27, 43, 5, 49,
        33, 9, 42, 19, 29, 28, 14, 39, 12, 38, 41, 13, 37, 48, 7, 16, 24, 55, 40,
        61, 26, 17, 0, 1, 60, 51, 30, 4, 22, 25, 54, 21, 56, 59, 6, 63, 57, 62, 11,
        36, 20, 34, 44, 52
    ]
    return functools.reduce(lambda s, i: s + orig[i], mixin_key_enc_tab, '')[:32]

def enc_wbi(params: dict, img_key: str, sub_key: str):
    'ä¸ºè¯·æ±‚å‚æ•°è¿›è¡Œ wbi ç­¾å'
    mixin_key = get_mixin_key(img_key + sub_key)
    curr_time = round(time.time())
    params['wts'] = curr_time # æ·»åŠ æ—¶é—´æˆ³
    # æŒ‰ç…§ key é‡æ’å‚æ•°
    params = dict(sorted(params.items()))
    # è¿‡æ»¤ä¸ç”¨ç­¾åçš„å­—ç¬¦
    query = urlencode(params)
    # è®¡ç®— w_rid
    w_rid = hashlib.md5((query + mixin_key).encode(encoding='utf-8')).hexdigest()
    params['w_rid'] = w_rid
    return params

def get_wbi_keys():
    'è·å–æœ€æ–°çš„ img_key å’Œ sub_key'
    try:
        resp = requests.get('https://api.bilibili.com/x/web-interface/nav', headers={
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        })
        resp.raise_for_status()
        json_content = resp.json()
        img_url = json_content['data']['wbi_img']['img_url']
        sub_url = json_content['data']['wbi_img']['sub_url']
        img_key = img_url.rsplit('/', 1)[1].split('.')[0]
        sub_key = sub_url.rsplit('/', 1)[1].split('.')[0]
        return img_key, sub_key
    except Exception as e:
        print(f"âš ï¸ æ— æ³•è·å– WBI å¯†é’¥: {e}")
        return None, None

# ============================================================
# é…ç½®åŒº â€” ä¿®æ”¹è¿™é‡Œæ¥é€‚é…ä½ è‡ªå·±çš„ RSSHub å®ä¾‹
# ============================================================
RSSHUB_BASE = os.environ.get("RSSHUB_BASE", "https://rsshub.app")

# æ•°æ®æ–‡ä»¶è·¯å¾„
DATA_FILE = os.path.join(os.path.dirname(os.path.abspath(__file__)), "news_data.json")

# æ»šåŠ¨æ›´æ–°ç­–ç•¥ï¼šæ¯ä¸ªåˆ†ç±»æœ€å¤šä¿ç•™çš„æ¡ç›®æ•°
MAX_ITEMS_PER_CATEGORY = 50

# æ•°æ®ä¿ç•™å¤©æ•°
MAX_AGE_DAYS = 30

# è¯·æ±‚è¶…æ—¶ï¼ˆç§’ï¼‰
REQUEST_TIMEOUT = 30

# ä¸œæ–¹ç›¸å…³å…³é”®è¯ 2.0 Proç‰ˆï¼ˆåˆ†ç±»ç®¡ç† + é»‘åå•æœºåˆ¶ï¼‰
# --- æ ¸å¿ƒå…³é”®è¯ï¼šå‡ºç°ä»»æ„ä¸€ä¸ªå³å¯åˆ¤å®šä¸ºä¸œæ–¹ç›¸å…³ ---
CORE_KEYWORDS = [
    "ä¸œæ–¹project", "æ±æ–¹project", "touhou project", "touhou",
    # æ—¥æ–‡å‡åä¸ç‰‡å‡åï¼Œæå‡æ—¥æ–‡/æ—¥åŒºå¹³å°å‘½ä¸­ç‡
    "ãƒˆã‚¦ãƒ›ã‚¦", "ã¨ã†ã»ã†",
    "å¹»æƒ³ä¹¡", "å¹»æƒ³éƒ·", "gensokyo",
    "åšä¸½ç¥ç¤¾", "åšéº—ç¥ç¤¾", "hakurei",
    "ZUN", "ä¸Šæµ·çˆ±ä¸½ä¸", "ä¸Šæµ·ã‚¢ãƒªã‚¹å¹»æ¨‚å›£",
    "ä¾‹å¤§ç¥­", "reitaisai",
    "thwiki", "THBWiki", "ä¸œæ–¹å§","ä¸œæ–¹MMD",
]

# --- è§’è‰²å…³é”®è¯ ---
CHARACTER_KEYWORDS = [
    "çµæ¢¦", "éœŠå¤¢", "reimu",
    "é­”ç†æ²™", "marisa",
    "å’²å¤œ", "sakuya",
    "çªéœ²è¯º", "ãƒãƒ«ãƒ", "cirno",
    "å¦–æ¢¦", "å¦–å¤¢", "youmu",
    "å¹½å¹½å­", "yuyuko",
    "è•¾ç±³è‰äºš", "remilia",
    "èŠ™å…°æœµéœ²", "flandre",
    "å¸•ç§‹è‰", "patchouli",
    "å°„å‘½ä¸¸æ–‡", "aya shameimaru",
    "æ²³åŸè·å–", "nitori",
    "å…«äº‘ç´«", "å…«é›²ç´«", "yukari",
    "è—¤åŸå¦¹çº¢", "mokou",
    "é¬¼äººæ­£é‚ª", "seija",
    "å¤æ˜åœ°è§‰", "å¤æ˜åœ°æ‹", "satori", "koishi",
    "é£è§å¹½é¦™", "yuuka",
    "å››å­£æ˜ å§¬", "eiki",
    "å°é‡å¡šå°ç”º", "komachi",
    "å› å¹¡å¸", "tewi",
    "é“ƒä»™", "éˆ´ä»™", "reisen",
    "æ°¸ç³", "eirin",
    "è¾‰å¤œ", "è¼å¤œ", "kaguya",
    "çº¢ç¾é“ƒ", "meiling",
    "çˆ±ä¸½ä¸", "alice margatroid",
    "è¥¿è¡Œå¯º", "saigyouji",
    "åšä¸½", "åšéº—",
    "å…«äº‘è“", "å…«é›²è—", "ran",
    "éœ²å¨œåˆ‡éœ²å¾·", "luna child",
]

# --- ä½œå“å…³é”®è¯ ---
GAME_KEYWORDS = [
    "çº¢é­”ä¹¡", "ç´…é­”éƒ·", "çº¢é­”é¦†", "ç´…é­”é¤¨",
    "å¦–å¦–æ¢¦", "å¦–ã€…å¤¢",
    "æ°¸å¤œæŠ„",
    "èŠ±æ˜ å¡š",
    "é£ç¥å½•", "é¢¨ç¥éŒ²",
    "åœ°çµæ®¿", "åœ°éœŠæ®¿",
    "æ˜Ÿè²èˆ¹", "æ˜Ÿè“®èˆ¹",
    "ç¥çµåº™", "ç¥éœŠå»Ÿ",
    "è¾‰é’ˆåŸ", "è¼é‡åŸ",
    "ç»€ç ä¼ ", "ç´ºç ä¼",
    "å¤©ç©ºç’‹",
    "é¬¼å½¢å…½", "é¬¼å½¢ç£",
    "è™¹é¾™æ´", "è™¹é¾æ´",
    "å…½ç‹å›­", "ç£ç‹åœ’",
    "çŒ®åæŠ„",
    "åˆšæ¬²å¼‚é—»",
    "ä¸œæ–¹çº¢é­”ä¹¡", "ä¸œæ–¹å¦–å¦–æ¢¦", "ä¸œæ–¹æ°¸å¤œæŠ„",
    "ä¸œæ–¹é£ç¥å½•", "ä¸œæ–¹åœ°çµæ®¿", "ä¸œæ–¹æ˜Ÿè²èˆ¹",
    "ä¸œæ–¹ç¥çµåº™", "ä¸œæ–¹è¾‰é’ˆåŸ", "ä¸œæ–¹ç»€ç ä¼ ",
    "ä¸œæ–¹å¤©ç©ºç’‹", "ä¸œæ–¹é¬¼å½¢å…½", "ä¸œæ–¹è™¹é¾™æ´",
    "ä¸œæ–¹å…½ç‹å›­", "ä¸œæ–¹çŒ®åæŠ„", "ä¸œæ–¹åˆšæ¬²å¼‚é—»",
]

# --- éŸ³ä¹/äºŒåˆ›å…³é”®è¯ ---
MUSIC_KEYWORDS = [
    "ä¸œæ–¹arrange", "ä¸œæ–¹ç¼–æ›²", "ä¸œæ–¹åŒäººéŸ³ä¹",
    "U.N.ã‚ªãƒ¼ã‚¨ãƒ³ã¯å½¼å¥³ãªã®ã‹", "ãƒã‚¯ãƒ­ãƒ•ã‚¡ãƒ³ã‚¿ã‚¸ã‚¢",
    "bad apple", "è‰²ã¯åŒ‚ã¸ã©æ•£ã‚Šã¬ã‚‹ã‚’",
    "ä¸œæ–¹vocal", "ä¸œæ–¹remix",
    "ç§˜å°ä¿±ä¹éƒ¨", "ç§˜å°å€¶æ¥½éƒ¨",
]

# ============================================================
# â›” é»‘åå• (Blacklist) - çœ‹åˆ°è¿™äº›è¯ç›´æ¥ä¸¢å¼ƒ
# ============================================================
BLACKLIST_KEYWORDS = [
    # ç«å“æ¸¸æˆ IP (MMDåŒºçš„å¤§å¤´)
    "åŸç¥", "Genshin", "ç±³å“ˆæ¸¸", "miHoYo", "æç“¦ç‰¹",
    "å´©å", "Honkai", "æ˜Ÿç©¹é“é“", "StarRail", "Star Rail", "ç»åŒºé›¶", "ZZZ",
    "æ˜æ—¥æ–¹èˆŸ", "Arknights", "é¹°è§’", "Hypergryph", "æ³°æ‹‰å¤§é™†",
    "ç¢§è“æ¡£æ¡ˆ", "BlueArchive", "Blue Archive", "è”šè“æ¡£æ¡ˆ",
    "ç‹è€…è£è€€", "LOL", "è‹±é›„è”ç›Ÿ", "æ°¸åŠ«æ— é—´", "Naraka",
    "ç¬¬äº”äººæ ¼", "é˜´é˜³å¸ˆ", "èµ›é©¬å¨˜",
    "Fate", "FGO", "Fate/Grand Order",
    "è¶…æ—¶ç©ºè¾‰å¤œå§¬", "è¶…æ™‚ç©ºè¼å¤œå§«", # ç›¸åŒçš„ä¼ è¯´åŸè®¾ä½†æ˜¯å…¶å®ä¸ç›¸å…³
    
    # è™šæ‹Ÿä¸»æ’­ (Vtubers ç»å¸¸å’Œ MMD æ··åœ¨ä¸€èµ·)
    "Hololive", "Nijisanji", "Asoul", "å˜‰ç„¶", "è´æ‹‰", 
    "åˆéŸ³", "Miku", "æ´›å¤©ä¾", "Vocaloid", # é™¤éå’Œä¸œæ–¹æ··æ­ï¼Œå¦åˆ™è¿‡æ»¤

    # æ— å…³å…³é”®è¯
    "äº’åŠ¨è§†é¢‘", "æŠ½å¥–", "æµ‹è¯•", "ä½œä¸š", "è¯¾å ‚", "æ•™ç¨‹",
]

# åˆå¹¶ä¸ºæ€»å…³é”®è¯åˆ—è¡¨
TOUHOU_KEYWORDS = CORE_KEYWORDS + CHARACTER_KEYWORDS + GAME_KEYWORDS + MUSIC_KEYWORDS

# ============================================================
# RSS æºé…ç½®
# ============================================================
RSS_SOURCES = {
    # === å¤´ç‰ˆå¤´æ¡ (Official) ===
    "official": {
        "label": "å¤´ç‰ˆå¤´æ¡",
        "feeds": [
            {
                "name": "ä¸œæ–¹å®˜æ–¹èµ„è®¯ç«™",
                # ä¼˜å…ˆä½¿ç”¨åŸç”Ÿ WordPress feedï¼Œç»•è¿‡ RSSHub
                "url": "https://touhou-project.news/feed.rss",
                "icon": "ğŸ“°",
                "priority": 1,
            },

        ],
    },

    # === ç¤¾ä¼š/æ°‘ç”Ÿ (Community) ===
    "community": {
        "label": "ç¤¾ä¼šÂ·æ°‘ç”Ÿ",
        "feeds": [
            {
                "name": "Reddit r/touhou",
                # ç›´æ¥ä½¿ç”¨ Reddit åŸç”Ÿ RSS
                "url": "https://www.reddit.com/r/touhou/new/.rss",
                "icon": "ğŸ’¬",
                "priority": 2,
            },
        ],
    },

    # === è‰ºæœ¯/å‰¯åˆŠ (Art) ===
    "art": {
        "label": "è‰ºæœ¯Â·å‰¯åˆŠ",
        "feeds": [
            # Safebooru å·²æ”¹ä¸º API è°ƒç”¨ï¼Œæ­¤å¤„ç•™ç©ºæˆ–ä¿ç•™å…¶ä»–RSSæº
        ],
    },
}


# ============================================================
# å·¥å…·å‡½æ•°
# ============================================================


def generate_id(title: str, link: str) -> str:
    """æ ¹æ®æ ‡é¢˜å’Œé“¾æ¥ç”Ÿæˆå”¯ä¸€ ID"""
    raw = f"{title}|{link}"
    return hashlib.md5(raw.encode("utf-8")).hexdigest()[:12]


def is_touhou_related(text: str) -> bool:
    """
    åˆ¤æ–­æ–‡æœ¬æ˜¯å¦ä¸ä¸œæ–¹ç›¸å…³ (é»‘åå•ä¼˜å…ˆç­–ç•¥)
    """
    if not text:
        return False
    text_lower = text.lower()
    
    # 1. âš”ï¸ é»‘åå•æ£€æŸ¥ (ä¸€ç¥¨å¦å†³)
    # åªè¦å‡ºç°äº†ç«å“è¯æ±‡ï¼Œç›´æ¥åˆ¤æ­»åˆ‘ï¼Œé™¤éå®ƒæ˜ç¡®æ ‡è®°äº†æ˜¯â€œä¸œæ–¹Projectâ€çš„æ··åˆäºŒåˆ›
    for bad_word in BLACKLIST_KEYWORDS:
        if bad_word.lower() in text_lower:
            # å”¯ä¸€çš„â€œè±å…æƒâ€ï¼šå¦‚æœæ ‡é¢˜é‡ŒåŒæ—¶ç¡¬æ ¸åœ°å†™äº† "ä¸œæ–¹" æˆ– "Touhou"
            # (é˜²æ­¢è¯¯æ€æ¯”å¦‚ "ä¸œæ–¹ x åŸç¥" çš„è·¨ç•Œæ•´æ´»)
            if "ä¸œæ–¹" in text_lower or "æ±æ–¹" in text_lower or "touhou" in text_lower:
                continue 
            
            # è°ƒè¯•æ—¥å¿—ï¼šè®©ä½ çŸ¥é“æ˜¯è°è¢«æ€æ‰äº†
            # print(f"       [é»‘åå•æ‹¦æˆª] å‘ç°å…³é”®è¯: {bad_word}") 
            return False

    # 2. âœ… æ­£å‘å…³é”®è¯æ£€æŸ¥
    # åªè¦å‘½ä¸­ä¸€ä¸ªæ­£å‘è¯ï¼Œå°±è®¤ä¸ºæ˜¯ä¸œæ–¹ç›¸å…³
    for kw in TOUHOU_KEYWORDS:
        if kw.lower() in text_lower:
            return True
            
    return False


def is_important_zun_tweet(text: str) -> bool:
    """åˆ¤æ–­ ZUN çš„æ¨ç‰¹æ˜¯å¦åŒ…å«é‡è¦ä¿¡æ¯ï¼ˆç”¨äº is_zun æ ‡è®°æºï¼‰ã€‚

    ç­–ç•¥ï¼šåŸºäºå…³é”®è¯åŠ æƒï¼ŒåŒ…å«å‘å¸ƒ/å¼€å‘/ä¾‹å¤§ç¥­/å…¬å¼€ç­‰è¯è§†ä¸ºé‡è¦ï¼›
    åŒæ—¶å¦‚æœå¸¦å›¾ç‰‡ä¹Ÿå¯è§†ä½œè¾ƒé‡è¦çš„åŠ¨æ€ã€‚
    """
    if not text:
        return False
    text_lower = text.lower()

    keywords = [
        "æ–°ä½œ", "ä½“é¨“ç‰ˆ", "ä½“é¨“", "å®Œæˆ", "å…¥ç¨¿", "ç™¼å”®", "å…¬é–‹", "å‘å¸ƒ", "ç™ºå£²", "ç™ºè¡¨", "å‘ŠçŸ¥", "ãƒªãƒªãƒ¼ã‚¹",
        "ä¾‹å¤§ç¥­", "ã‚³ãƒŸã‚±", "å¤ã‚³ãƒŸ", "å†¬ã‚³ãƒŸ", "reitaisai",
        "release", "steam", "é…ä¿¡", "å…¬é–‹", "interview", "ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼",
        # æ—¥æ–‡å‡å/ç‰‡å‡åä¸è‹±æ–‡
        "ãƒˆã‚¦ãƒ›ã‚¦", "ã¨ã†ã»ã†", "touhou", "æ±æ–¹", "touhou project", "æ±æ–¹project",
    ]

    for kw in keywords:
        if kw.lower() in text_lower:
            return True

    # å¦‚æœåŒ…å«å›¾ç‰‡æ ‡ç­¾ï¼Œé€šå¸¸ä¹Ÿæ¯”è¾ƒå€¼å¾—å…³æ³¨
    if "<img" in text_lower:
        return True

    return False


def clean_html(raw_html: str) -> str:
    """ç§»é™¤ HTML æ ‡ç­¾ï¼Œä¿ç•™çº¯æ–‡æœ¬"""
    if not raw_html:
        return ""
    clean = re.sub(r"<[^>]+>", "", raw_html)
    clean = re.sub(r"\s+", " ", clean).strip()
    return clean[:300]  # æ‘˜è¦æˆªæ–­


def extract_image(entry) -> Optional[str]:
    """ä» RSS æ¡ç›®ä¸­å°½åŠ›æå–ä¸€å¼ å›¾ç‰‡ URL"""
    # å°è¯• media:content
    if hasattr(entry, "media_content") and entry.media_content:
        for media in entry.media_content:
            if "image" in media.get("type", "") or media.get("url", "").endswith(
                (".jpg", ".png", ".webp", ".gif")
            ):
                return media["url"]

    # å°è¯• media:thumbnail
    if hasattr(entry, "media_thumbnail") and entry.media_thumbnail:
        return entry.media_thumbnail[0].get("url")

    # å°è¯• enclosure
    if hasattr(entry, "enclosures") and entry.enclosures:
        for enc in entry.enclosures:
            if "image" in enc.get("type", ""):
                return enc.get("href") or enc.get("url")

    # å°è¯•ä» description/content ä¸­æå– <img>
    content = ""
    if hasattr(entry, "content") and entry.content:
        content = entry.content[0].get("value", "")
    elif hasattr(entry, "summary"):
        content = entry.summary or ""

    img_match = re.search(r'<img[^>]+src=["\']([^"\']+)["\']', content)
    if img_match:
        return img_match.group(1)

    return None


def fetch_bilibili_rank_api(rid: int, label: str) -> list:
    """
    [APIç›´è¿] è·å– Bç«™æŒ‡å®šåˆ†åŒºçš„æ’è¡Œæ¦œæ•°æ® (åŠ å¼ºä¼ªè£…ç‰ˆ + WBIç­¾å)
    """
    # 1. å…ˆæ‹¿åˆ°å¯†é’¥
    img_key, sub_key = get_wbi_keys()
    if not img_key: 
        print("  âš  WBI ç­¾åå¯†é’¥è·å–å¤±è´¥ï¼Œè·³è¿‡ Bç«™è¯·æ±‚")
        return []

    # 2. å‡†å¤‡åŸå§‹å‚æ•°
    params = {
        'rid': rid,
        'type': 'all',
        # 'web_location': '333.999', # æœ‰æ—¶å€™éœ€è¦è¿™ä¸ª
    }
    
    # 3. ç­¾åï¼
    signed_params = enc_wbi(params, img_key, sub_key)

    api_url = f"https://api.bilibili.com/x/web-interface/ranking/v2"
    
    # ç”ŸæˆéšæœºæŒ‡çº¹ (ä¿æŒæ—§æœ‰çš„ Headers ä¼ªè£…ä½œä¸ºè¾…åŠ©)
    buvid3 = str(uuid.uuid4()) + "infoc"
    _uuid = str(uuid.uuid4())
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
        "Referer": "https://www.bilibili.com/v/popular/rank/all",
        "Origin": "https://www.bilibili.com",
        "Accept": "application/json, text/plain, */*",
        "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
        # æ¨¡æ‹Ÿæµè§ˆå™¨ç¯å¢ƒå¤´
        "Sec-Ch-Ua": '"Not A(Brand";v="99", "Google Chrome";v="121", "Chromium";v="121"',
        "Sec-Ch-Ua-Mobile": "?0",
        "Sec-Ch-Ua-Platform": '"Windows"',
        "Cookie": f"buvid3={buvid3}; _uuid={_uuid};" 
    }
    
    print(f"  âš¡ æ­£åœ¨è¯·æ±‚ Bç«™ API (åˆ†åŒº {rid}) [WBIç­¾åç‰ˆ]...")
    try:
        # requests ä¼šè‡ªåŠ¨å¸®ä½ æŠŠ signed_params æ‹¼æ¥åˆ° url åé¢
        resp = requests.get(api_url, headers=headers, params=signed_params, timeout=15)
        
        if resp.status_code != 200:
            print(f"  âŒ HTTP çŠ¶æ€ç é”™è¯¯: {resp.status_code}")
            return []

        data = resp.json()
        
        if data["code"] != 0:
            print(f"  âŒ Bç«™ API æ‹’ç»: Code {data['code']} - {data.get('message', 'æœªçŸ¥é”™è¯¯')}")
            return []
            
        items = []
        data_list = data.get("data", {}).get("list", [])
        
        for v in data_list[:15]:
            title = v["title"]
            desc = v.get("desc", "") or v.get("dynamic", "") or ""
            
            # å…³é”®è¯è¿‡æ»¤
            combined_text = title + " " + desc
            if not is_touhou_related(combined_text):
                continue
                
            items.append({
                "id": generate_id(v["bvid"], "bilibili"),
                "title": v["title"],
                "link": f"https://www.bilibili.com/video/{v['bvid']}",
                "summary": desc[:80].replace("\n", " ") + "...",
                "image": v["pic"].replace("http://", "https://") if "pic" in v else None,
                "source": f"Bç«™ {label}æ¦œ",
                "source_icon": "ğŸ“º",
                "priority": 1,
                "published": datetime.now(timezone.utc).isoformat(),
                "fetched_at": datetime.now(timezone.utc).isoformat(),
            })
        return items
    except Exception as e:
        print(f"  âš  Bç«™ API è¯·æ±‚å¼‚å¸¸: {e}")
        return []


def fetch_safebooru_api(tags: str = "touhou") -> list:
    """
    [APIç›´è¿] è·å– Safebooru å›¾ç‰‡åˆ—è¡¨ (JSON)
    """
    # json=1 è¡¨ç¤ºè¿”å› JSON æ ¼å¼
    # â¬†ï¸ æé«˜äº†å•æ¬¡æŠ“å–æ•°é‡ (10 -> 40)ï¼Œä»¥å¹³è¡¡é¡µé¢é«˜åº¦ï¼Œè®©å³ä¾§ä¸æ˜¾å¾—å¤ªç©º
    api_url = f"https://safebooru.org/index.php?page=dapi&s=post&q=index&json=1&tags={tags}&limit=40"
    headers = {"User-Agent": "GensokyoDaily/1.0"}
    
    print(f"  âš¡ æ­£åœ¨è¯·æ±‚ Safebooru API...")
    try:
        resp = requests.get(api_url, headers=headers, timeout=10)
        # Safebooru API æœ‰æ—¶è¿”å›ç©ºæˆ–éæ ‡å‡† JSONï¼Œéœ€è¦å°å¿ƒ
        if not resp.text.strip():
            return []
            
        data = resp.json()
        items = []
        
        for img in data:
            # æ„é€ å›¾ç‰‡ URL
            # Safebooru å›¾ç‰‡è·¯å¾„é€šå¸¸æ˜¯ images/{directory}/{image}
            image_url = f"https://safebooru.org/images/{img['directory']}/{img['image']}"
            post_url = f"https://safebooru.org/index.php?page=post&s=view&id={img['id']}"
            
            items.append({
                "id": str(img['id']),
                "title": f"Safebooru: {img['id']}", # å›¾ç«™é€šå¸¸æ²¡æ ‡é¢˜
                "link": post_url,
                "summary": f"Tags: {img['tags'][:50]}...",
                "image": image_url,
                "source": "Safebooru",
                "source_icon": "ğŸ¨",
                "priority": 2,
                "published": datetime.fromtimestamp(int(img.get('change', time.time())), tz=timezone.utc).isoformat(),
                "fetched_at": datetime.now(timezone.utc).isoformat(),
            })
        return items
    except Exception as e:
        print(f"  âš  Safebooru API è¯·æ±‚å¤±è´¥: {e}")
        return []


def parse_date(entry) -> str:
    """è§£æå‘å¸ƒæ—¶é—´ï¼Œè¿”å› ISO æ ¼å¼å­—ç¬¦ä¸²"""
    if hasattr(entry, "published_parsed") and entry.published_parsed:
        return datetime(*entry.published_parsed[:6], tzinfo=timezone.utc).isoformat()
    if hasattr(entry, "updated_parsed") and entry.updated_parsed:
        return datetime(*entry.updated_parsed[:6], tzinfo=timezone.utc).isoformat()
    return datetime.now(timezone.utc).isoformat()


def fetch_feed(url: str, timeout: int = REQUEST_TIMEOUT) -> Optional[feedparser.FeedParserDict]:
    """è·å–å¹¶è§£æ RSS feed"""
    try:
        # ä½¿ç”¨æµè§ˆå™¨ UA é¿å…è¢«é˜²ç«å¢™æ‹¦æˆª (å¦‚ THWiki)
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
            "Accept": "application/atom+xml,application/rss+xml,application/xml,text/xml;q=0.9,*/*;q=0.8"
        }
        with requests.Session() as session:
            resp = session.get(url, headers=headers, timeout=timeout)

        # å¦‚æœè¿”å›é 2xxï¼Œå°½é‡æ‰“å°æ›´å¤šä¿¡æ¯ä»¥ä¾¿æ’æŸ¥
        if resp.status_code >= 400:
            snippet = resp.text[:500].replace("\n", " ") if resp.text else ""
            print(f"  âš  è·å–å¤±è´¥: {url} â€” HTTP {resp.status_code} {resp.reason}")
            if snippet:
                print(f"    â†’ å“åº”ç‰‡æ®µ: {snippet}")
            return None

        parsed = feedparser.parse(resp.text)
        # feedparser æœ‰ bozo æ ‡å¿—è¡¨ç¤ºè§£ææ—¶å‡ºç°å¼‚å¸¸
        if getattr(parsed, "bozo", False):
            be = getattr(parsed, "bozo_exception", None)
            print(f"  âš  è§£æè­¦å‘Š: {url} â€” {be}")

        return parsed
    except requests.exceptions.RequestException as e:
        # requests å¼‚å¸¸æ—¶å°½é‡è¾“å‡ºçŠ¶æ€ä¸å“åº”ç‰‡æ®µï¼ˆå¦‚æœæœ‰ï¼‰
        msg = str(e)
        resp = getattr(e, "response", None)
        if resp is not None:
            try:
                snippet = resp.text[:500].replace("\n", " ")
            except Exception:
                snippet = "(unable to read response body)"
            print(f"  âš  è·å–å¤±è´¥: {url} â€” HTTP {resp.status_code} {resp.reason} â€” {msg}")
            print(f"    â†’ å“åº”ç‰‡æ®µ: {snippet}")
        else:
            print(f"  âš  è·å–å¤±è´¥: {url} â€” {msg}")
        return None
    except Exception as e:
        print(f"  âš  è§£æå¤±è´¥: {url} â€” {e}")
        return None


def clean_html(raw_html: str) -> str:
    """å»é™¤ HTML æ ‡ç­¾"""
    if not raw_html:
        return ""
    cleanr = re.compile("<.*?>")
    text = re.sub(cleanr, "", raw_html)
    return text.strip()


def extract_image(entry) -> Optional[str]:
    """å°è¯•ä» feed entry ä¸­æå–å°é¢å›¾"""
    # 1. åª’ä½“é™„ä»¶ (Safebooru ç­‰)
    if "media_content" in entry:
        for m in entry.media_content:
            if m.get("medium") == "image":
                return m["url"]
    
    # 2. åª’ä½“ç¼©ç•¥å›¾ (YouTube ç­‰)
    if "media_thumbnail" in entry:
        return entry.media_thumbnail[0]["url"]
    
    # 3.  enclosure (WordPress ç­‰)
    if "enclosures" in entry:
        for enc in entry.enclosures:
            if enc.get("type", "").startswith("image/"):
                return enc.get("href")
            
    # 4. ä» description/summary çš„ HTML ä¸­æå– img æ ‡ç­¾
    content = entry.get("summary", "") or entry.get("description", "") or entry.get("content", [{"value": ""}])[0]["value"]
    soup_match = re.search(r'<img [^>]*src="([^"]+)"', content)
    if soup_match:
        return soup_match.group(1)
        
    return None

# ============================================================
# ğŸ› ï¸ æ ¸å¿ƒå‡½æ•°ï¼šä½¿ç”¨è€æ¥å£ç›´è¿ B ç«™
# ============================================================
def fetch_bilibili_partition_newlist(rid: int, partition_name: str) -> list:
    """
    [æˆ˜æœ¯å‡çº§] ä½¿ç”¨ /x/web-interface/newlist æ¥å£ (æœ€æ–°è§†é¢‘)
    ç­–ç•¥ï¼šä»¥é‡å–èƒœã€‚æ‹‰å–æœ€æ–° 50 æ¡è§†é¢‘ï¼Œæ€»æœ‰å‡ æ¡æ˜¯ä¸œæ–¹çš„ã€‚
    """
    # ps=50 è¡¨ç¤ºä¸€æ¬¡æ‹‰ 50 æ¡ (æœ€å¤§å€¼)
    api_url = f"https://api.bilibili.com/x/web-interface/newlist?rid={rid}&ps=50&pn=1"
    
    # ä¼ªé€  Cookie ä¾ç„¶æ˜¯å¿…é¡»çš„
    fake_buvid3 = str(uuid.uuid4()) + "infoc"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36",
        "Referer": "https://www.bilibili.com/",
        "Cookie": f"buvid3={fake_buvid3}; nostalgia_conf=-1"
    }
    
    print(f"    âš¡ æ­£åœ¨è¯·æ±‚åˆ†åŒº {rid} ({partition_name}) æœ€æ–°æŠ•ç¨¿...")
    
    try:
        resp = requests.get(api_url, headers=headers, timeout=10)
        
        if resp.status_code != 200:
            print(f"    âŒ HTTP Error: {resp.status_code}")
            return []

        data = resp.json()
        if data["code"] != 0:
            print(f"    âŒ ä¸šåŠ¡æ‹’ç»: {data['message']}")
            return []
            
        # è·å–è§†é¢‘åˆ—è¡¨ (æ–°æ¥å£ç»“æ„: data -> archives)
        video_list = data.get("data", {}).get("archives", [])
        
        if not video_list:
            print("    âš  è¿”å›åˆ—è¡¨ä¸ºç©º")
            return []

        print(f"    âœ… æˆåŠŸè·å– {len(video_list)} æ¡å€™é€‰è§†é¢‘ï¼Œå¼€å§‹ç­›é€‰...")
        
        items = []
        dropped_count = 0
        
        for v in video_list:
            title = v["title"]
            desc = v.get("desc", "") or ""
            # è·å–ä½œè€…åï¼Œå¢åŠ åˆ¤æ–­å‡†ç¡®åº¦
            author = v.get("owner", {}).get("name", "")
            
            # ç»„åˆæ£€æŸ¥ï¼šæ ‡é¢˜ + ç®€ä»‹ + ä½œè€…
            full_text = f"{title} {desc} {author}"
            
            if is_touhou_related(full_text):
                # å‘½ä¸­ï¼
                items.append({
                    "id": generate_id(v["bvid"], "bilibili_new"),
                    "title": title,
                    "link": f"https://www.bilibili.com/video/{v['bvid']}",
                    "summary": desc[:80].replace("\n", " ") + "...",
                    "image": v["pic"].replace("http:", "https:"),
                    "source": partition_name,
                    "source_icon": "ğŸ“º", # è¿™é‡Œä¹Ÿå¯ä»¥ç”¨ä¼ è¿›æ¥çš„ icon
                    "priority": 1,
                    "published": datetime.fromtimestamp(v["pubdate"], tz=timezone.utc).isoformat(),
                    "fetched_at": datetime.now(timezone.utc).isoformat(),
                })
            else:
                dropped_count += 1
                # æ‰“å°å‰3ä¸ªè¢«æ‰”æ‰çš„æ ‡é¢˜ï¼Œè®©ä½ çŸ¥é“å‘ç”Ÿäº†ä»€ä¹ˆ (è°ƒè¯•ç”¨)
                if dropped_count <= 3:
                    print(f"       [è¿‡æ»¤] æ‰”æ‰: {title[:20]}...")

        print(f"    ğŸ“Š ç­›é€‰ç»“æœ: {len(items)} æ¡å‘½ä¸­ / {len(video_list)} æ¡æ€»æ•°")
        return items

    except Exception as e:
        print(f"    âš  è¿æ¥å¼‚å¸¸: {e}")
        return []


def fetch_thwiki_api() -> list:
    """
    [APIç›´è¿] è·å– THWiki æœ€è¿‘æ›´æ”¹ (ä¼˜å…ˆç›´è¿ï¼Œå¤±è´¥è½¬ä»£ç† + é‡è¯•)
    """
    # 1. THWiki å®˜æ–¹ API å‚æ•°
    target_url = "https://thwiki.cc/api.php?action=query&list=recentchanges&rcnamespace=0&rcprop=title|ids|timestamp|user|comment&format=json&rclimit=10"
    
    # è¾…åŠ©å‡½æ•°ï¼šå¤„ç†æ•°æ®
    def process_data(data):
        items = []
        rc_list = data.get("query", {}).get("recentchanges", [])
        if not rc_list:
            return []
            
        for rc in rc_list:
            title = rc["title"]
            comment = rc.get("comment", "") or "æ— ç¼–è¾‘æ‘˜è¦"
            user = rc.get("user", "åŒ¿åç”¨æˆ·")
            # è¿‡æ»¤æœºå™¨äºº
            if "bot" in user.lower() or "Bot" in user:
                continue
            items.append({
                "id": f"thwiki_{rc['rcid']}",
                "title": f"ã€ç™¾ç§‘ã€‘{title}",
                "link": f"https://thwiki.cc/{requests.utils.quote(title)}",
                "summary": f"ç¼–è€…: {user}\nå¤‡æ³¨: {comment}",
                "image": None,
                "source": "THWiki",
                "source_icon": "ğŸ“š",
                "priority": 2,
                "published": rc["timestamp"],
                "fetched_at": datetime.now(timezone.utc).isoformat(),
            })
        return items

    # --- é˜¶æ®µ 1: å°è¯•ç›´è¿ ---
    print(f"  âš¡ æ­£åœ¨å°è¯•ç›´è¿ THWiki API...")
    try:
        # ç›´è¿é€šå¸¸å¾ˆå¿«ï¼Œæˆ–è€…ç›´æ¥ä¸é€šï¼Œæ‰€ä»¥è¶…æ—¶è®¾çŸ­ä¸€ç‚¹
        resp = requests.get(target_url, timeout=5, headers={
            "User-Agent": "GensokyoDaily/1.0 (Direct)"
        })
        if resp.status_code == 200:
            data = resp.json()
            items = process_data(data)
            if items:
                print(f"    âœ… ç›´è¿æˆåŠŸï¼è·å– {len(items)} æ¡æ•°æ®")
                return items
            else:
                print("    âš  ç›´è¿è¿”å›æ•°æ®ä¸ºç©ºï¼Œå°è¯•ä»£ç†...")
        else:
            print(f"    âš  ç›´è¿å¤±è´¥ (HTTP {resp.status_code})ï¼Œåˆ‡æ¢ä»£ç†...")
    except Exception as e:
        print(f"    âš  ç›´è¿å¼‚å¸¸ ({e})ï¼Œåˆ‡æ¢ä»£ç†...")

    # --- é˜¶æ®µ 2: ä»£ç†é‡è¯•æ¨¡å¼ ---
    proxy_url = f"https://api.allorigins.win/get?url={requests.utils.quote(target_url)}"
    print(f"  âš¡ å¯åŠ¨ Plan B: THWiki API (via AllOrigins)...")
    
    max_retries = 3
    for attempt in range(1, max_retries + 1):
        try:
            # â³ æŠŠè¶…æ—¶æ—¶é—´ä» 20s å»¶é•¿åˆ° 30s
            resp = requests.get(proxy_url, timeout=30)
            
            if resp.status_code != 200:
                print(f"    âš  [ç¬¬{attempt}æ¬¡] ä»£ç†è¿”å› HTTP {resp.status_code}ï¼Œé‡è¯•ä¸­...")
                time.sleep(2)
                continue
                
            wrapper_data = resp.json()
            if not wrapper_data.get("contents"):
                print(f"    âš  [ç¬¬{attempt}æ¬¡] ä»£ç†è¿”å›ç©ºå†…å®¹ï¼Œé‡è¯•ä¸­...")
                time.sleep(2)
                continue
                
            real_data = json.loads(wrapper_data["contents"])
            items = process_data(real_data)
            
            if not items:
                print("    âš  THWiki è¿”å›åˆ—è¡¨ä¸ºç©º")
                return []
                
            print(f"    âœ… ä»£ç†æˆåŠŸè·å– {len(items)} æ¡ç»´åŸºåŠ¨æ€")
            return items

        except Exception as e:
            print(f"    âš  [ç¬¬{attempt}æ¬¡] è¿æ¥å¼‚å¸¸: {e}")
            if attempt < max_retries:
                print("       ç­‰å¾… 5 ç§’åé‡è¯•...")
                time.sleep(5)
            else:
                print("    ğŸ’€ æœ€ç»ˆå¤±è´¥ï¼šTHWiki æ¥å£å¤šæ¬¡å°è¯•å‡è¶…æ—¶")
                return []
    return []

# ============================================================
# å¤©æ°”æ¨¡å—ï¼ˆè™šæ„ - å¹»æƒ³ä¹¡å¤©æ°”ï¼‰
# ============================================================


def generate_gensokyo_weather() -> dict:
    """ç”Ÿæˆå¹»æƒ³ä¹¡å„åœ°çš„è™šæ„å¤©æ°”"""
    import random

    locations = [
        {"name": "åšä¸½ç¥ç¤¾", "name_jp": "åšéº—ç¥ç¤¾"},
        {"name": "äººé—´ä¹‹é‡Œ", "name_jp": "äººé–“ã®é‡Œ"},
        {"name": "çº¢é­”é¦†", "name_jp": "ç´…é­”é¤¨"},
        {"name": "ç™½ç‰æ¥¼", "name_jp": "ç™½ç‰æ¥¼"},
        {"name": "æ°¸è¿œäº­", "name_jp": "æ°¸é äº­"},
        {"name": "å®ˆçŸ¢ç¥ç¤¾", "name_jp": "å®ˆçŸ¢ç¥ç¤¾"},
        {"name": "åœ°çµæ®¿", "name_jp": "åœ°éœŠæ®¿"},
        {"name": "å‘½è²å¯º", "name_jp": "å‘½è“®å¯º"},
    ]

    conditions = [
        {"text": "æ™´", "icon": "â˜€ï¸"},
        {"text": "å¤šäº‘", "icon": "â›…"},
        {"text": "é˜´", "icon": "â˜ï¸"},
        {"text": "å°é›¨", "icon": "ğŸŒ¦ï¸"},
        {"text": "é›·é˜µé›¨", "icon": "â›ˆï¸"},
        {"text": "å¼¹å¹•æš´é£", "icon": "ğŸŒ€"},
        {"text": "å¦–é›¾", "icon": "ğŸŒ«ï¸"},
        {"text": "èŠ±ç²‰", "icon": "ğŸŒ¸"},
        {"text": "å¼‚å˜ä¸­", "icon": "âš¡"},
        {"text": "çº¢é›¾", "icon": "ğŸŒ…"},
        {"text": "é›ª", "icon": "â„ï¸"},
        {"text": "æ¨±å¹é›ª", "icon": "ğŸŒ¸"},
    ]

    weather_data = []
    for loc in locations:
        cond = random.choice(conditions)
        temp = random.randint(-5, 35)
        weather_data.append(
            {
                "location": loc["name"],
                "location_jp": loc["name_jp"],
                "condition": cond["text"],
                "icon": cond["icon"],
                "temperature": temp,
            }
        )

    return {
        "updated": datetime.now(timezone.utc).isoformat(),
        "forecasts": weather_data,
    }


# ============================================================
# ä¸»æŠ“å–é€»è¾‘
# ============================================================


def fetch_all_news() -> dict:
    """æŠ“å–æ‰€æœ‰åˆ†ç±»çš„æ–°é—»"""
    print("=" * 60)
    print("ğŸ—ï¸  å¹»æƒ³ä¹¡æ—¥æŠ¥ â€” å¼€å§‹æŠ“å–æ–°é—»")
    print(f"ğŸ”— ä½¿ç”¨ RSSHUB_BASE: {RSSHUB_BASE}")
    print(f"ğŸ“…  {datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')}")
    print("=" * 60)

    all_news = {}
    cutoff_date = datetime.now(timezone.utc) - timedelta(days=MAX_AGE_DAYS)

    for category_key, category_config in RSS_SOURCES.items():
        print(f"\nğŸ“‚ åˆ†ç±»: {category_config['label']}")
        items = []

        # ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœæ˜¯ community åˆ†ç±»ï¼Œå…ˆæ’å…¥ B ç«™åˆ†åŒºæ•°æ®
        if category_key == "community":
            print(f"  ğŸ‘‰ å¯åŠ¨ Bç«™åˆ†åŒºæŠ“å–å­ç³»ç»Ÿ (Newlist æ¦‚ç‡å­¦æ¨¡å¼)...")
            bili_items = []
            for part in BILIBILI_PARTITIONS:
                print(f"  ğŸ”— æ­£åœ¨æŠ“å–: {part['name']}")
                
                # è°ƒç”¨æ–°å‡½æ•°ï¼šfetch_bilibili_partition_newlist
                part_items = fetch_bilibili_partition_newlist(part['rid'], part['name'])
                
                if part_items:
                    for item in part_items:
                        item["source_icon"] = part["icon"] # è¡¥ä¸Šå›¾æ ‡
                        item["category"] = "community"
                    bili_items.extend(part_items)
                else:
                    print(f"  âš ï¸ åˆ†åŒº {part['name']} æš‚æ— å‘½ä¸­")
            
            print(f"  âœ… Bç«™åˆ†åŒºæŠ“å–ç»“æŸï¼Œå…± {len(bili_items)} æ¡æ•°æ®å¾…åˆå¹¶")
            items.extend(bili_items)

        for feed_config in category_config["feeds"]:
            print(f"  ğŸ”— æ­£åœ¨è·å–: {feed_config['name']}")
            feed = fetch_feed(feed_config["url"])

            if not feed or not feed.entries:
                print(f"  âš  æ— æ•°æ®æˆ–è·å–å¤±è´¥")
                continue

            count = 0
            for entry in feed.entries:
                title = entry.get("title", "").strip()
                link = entry.get("link", "").strip()
                if not title or not link:
                    continue

                # éœ€è¦è¿‡æ»¤çš„æºï¼šæ£€æŸ¥æ˜¯å¦ä¸ä¸œæ–¹ç›¸å…³
                if feed_config.get("needs_filter"):
                    summary_text = clean_html(
                        entry.get("summary", "") + " " + title
                    )
                    if not is_touhou_related(summary_text):
                        continue

                # ZUN ä¸“å±è¿‡æ»¤ï¼šå¯¹æ ‡è®°ä¸º is_zun çš„æºåšé‡è¦æ€§åˆ¤æ–­
                if feed_config.get("is_zun"):
                    full_text = clean_html(entry.get("summary", "") + " " + title)
                    if not is_important_zun_tweet(full_text):
                        continue

                item = {
                    "id": generate_id(title, link),
                    "title": title,
                    "link": link,
                    "summary": clean_html(entry.get("summary", "")),
                    "image": extract_image(entry),
                    "source": feed_config["name"],
                    "source_icon": feed_config["icon"],
                    "priority": feed_config["priority"],
                    "published": parse_date(entry),
                    "fetched_at": datetime.now(timezone.utc).isoformat(),
                }

                items.append(item)
                count += 1

            print(f"  âœ… è·å–åˆ° {count} æ¡")

        # å°†itemsåˆå¹¶åˆ°all_newsä¸­
        all_news[category_key] = {
            "label": category_config["label"],
            "items": items,
            "count": len(items),
        }

    # === 3. [æ–°å¢] ä¸“é—¨è°ƒç”¨ THWiki API ===
    print(f"\nğŸ“‚ åˆ†ç±»: ç™¾ç§‘åŠ¨æ€ (THWiki API)")
    wiki_items = fetch_thwiki_api()
    
    if wiki_items:
        # æŠŠç»´åŸºæ•°æ®ä¹Ÿåˆå¹¶åˆ° community (ç¤¾ä¼šÂ·æ°‘ç”Ÿ) ç‰ˆå—
        if "community" not in all_news:
            all_news["community"] = {"label": "ç¤¾ä¼šÂ·æ°‘ç”Ÿ", "items": [], "count": 0}
        
        all_news["community"]["items"].extend(wiki_items)
        all_news["community"]["count"] += len(wiki_items)

    # === 4. [æ–°å¢] ä¸“é—¨è°ƒç”¨ Safebooru API ===
    print(f"\nğŸ“‚ åˆ†ç±»: è‰ºæœ¯Â·å‰¯åˆŠ (Safebooru API)")
    safe_items = fetch_safebooru_api("touhou")
    print(f"  âœ… Safebooru API è·å– {len(safe_items)} æ¡")
    
    # å°† Safebooru æ•°æ®åˆå¹¶åˆ° art åˆ†ç±»ä¸­
    if "art" not in all_news:
        all_news["art"] = {"label": "è‰ºæœ¯Â·å‰¯åˆŠ", "items": [], "count": 0}
    all_news["art"]["items"].extend(safe_items)
    all_news["art"]["count"] += len(safe_items)

    # === 4. å¯¹æ‰€æœ‰åˆ†ç±»è¿›è¡Œç»Ÿä¸€çš„å»é‡ã€æ’åºã€æˆªæ–­ ===
    for category_key, category_data in all_news.items():
        original_items = category_data["items"]
        
        # å»é‡ï¼ˆæŒ‰ idï¼‰
        seen_ids = set()
        unique_items = []
        for item in original_items:
            if item["id"] not in seen_ids:
                seen_ids.add(item["id"])
                unique_items.append(item)

        # æŒ‰ä¼˜å…ˆçº§ï¼ˆæ•°å€¼è¶Šå°ä¼˜å…ˆçº§è¶Šé«˜ï¼‰å’Œå‘å¸ƒæ—¶é—´é™åºæ’åº
        # é¦–å…ˆæŠŠå‘å¸ƒæ—¶é—´è§£æä¸ºæ—¶é—´æˆ³ï¼Œç¡®ä¿æ’åºè¡Œä¸ºæ­£ç¡®
        def _ts(item):
            try:
                return datetime.fromisoformat(item.get("published", "")).timestamp()
            except Exception:
                return 0

        # key: (priority asc, published_ts desc)
        unique_items.sort(key=lambda x: (x.get("priority", 99), -_ts(x)))

        # æˆªæ–­åˆ°æœ€å¤§æ¡ç›®æ•°
        unique_items = unique_items[:MAX_ITEMS_PER_CATEGORY]

        # æ›´æ–° category_data
        category_data["items"] = unique_items
        category_data["count"] = len(unique_items)
        
        print(f"  ğŸ“Š åˆ†ç±» [{category_data['label']}] æœ€ç»ˆæ”¶å½• {len(unique_items)} æ¡")

    return all_news


def merge_with_existing(new_data: dict) -> dict:
    """
    ä¸å·²æœ‰æ•°æ®åˆå¹¶ï¼Œå®ç°å¢é‡æ›´æ–°ã€‚
    æ–°æ•°æ®è¦†ç›–åŒ id çš„æ—§æ•°æ®ï¼ŒåŒæ—¶ä¿ç•™æœªè¿‡æœŸçš„æ—§æ¡ç›®ã€‚
    """
    if not os.path.exists(DATA_FILE):
        return new_data

    try:
        with open(DATA_FILE, "r", encoding="utf-8") as f:
            existing = json.load(f)
    except (json.JSONDecodeError, IOError):
        return new_data

    existing_categories = existing.get("categories", {})
    cutoff = datetime.now(timezone.utc) - timedelta(days=MAX_AGE_DAYS)

    for cat_key, cat_data in new_data.items():
        new_items = {item["id"]: item for item in cat_data["items"]}

        # ä»æ—§æ•°æ®ä¸­ä¿ç•™æœªè¿‡æœŸä¸”ä¸é‡å¤çš„æ¡ç›®
        if cat_key in existing_categories:
            for old_item in existing_categories[cat_key].get("items", []):
                if old_item["id"] not in new_items:
                    try:
                        pub_date = datetime.fromisoformat(old_item["published"])
                        if pub_date.tzinfo is None:
                            pub_date = pub_date.replace(tzinfo=timezone.utc)
                        if pub_date > cutoff:
                            new_items[old_item["id"]] = old_item
                    except (ValueError, KeyError):
                        pass

        merged_list = list(new_items.values())
        merged_list.sort(key=lambda x: x.get("published", ""), reverse=True)
        merged_list = merged_list[:MAX_ITEMS_PER_CATEGORY]

        cat_data["items"] = merged_list
        cat_data["count"] = len(merged_list)

    return new_data


def main():
    """ä¸»å…¥å£"""
    start_time = time.time()

    # 1. æŠ“å–æ–°é—»
    news_data = fetch_all_news()

    # 2. åˆå¹¶æ—§æ•°æ®
    news_data = merge_with_existing(news_data)

    # 3. ç”Ÿæˆå¤©æ°”
    weather = generate_gensokyo_weather()

    # 4. è™šæ„å¹¿å‘Š
    ads = [
        {
            "id": "ad_kappa",
            "title": "æ²³ç«¥é‡å·¥ æœ€æ–°ç§‘æŠ€",
            "subtitle": "å…‰å­¦è¿·å½©ã€ç­‰ç¦»å­ç‚®ã€è‡ªåŠ¨é’“é±¼æœº",
            "description": "æ²³åŸè·å–é¢†è¡”ç ”å‘ï¼å¦–æ€ªå±±æ²³ç«¥å·¥ä¸šè”åˆä½“ï¼Œä¸ºæ‚¨æä¾›æœ€å‰æ²¿çš„å¹»æƒ³ç§‘æŠ€ã€‚æ¥æ–™åŠ å·¥ã€å®šåˆ¶å¼¹å¹•ç³»ç»Ÿï¼Œæ¬¢è¿å’¨è¯¢ã€‚",
            "contact": "å¦–æ€ªå±±ç€‘å¸ƒæ— æ²³ç«¥å·¥åŠ",
            "icon": "ğŸ”§",
        },
        {
            "id": "ad_eientei",
            "title": "æ°¸è¿œäº­ ç‰¹ä¾›è¯å‰‚",
            "subtitle": "å…«æ„æ°¸ç³ç›‘åˆ¶ Â· è“¬è±ä¹‹è¯é™¤å¤–",
            "description": "æ„Ÿå†’çµã€è·Œæ‰“ä¸¸ã€å¼¹å¹•åˆ›ä¼¤é€Ÿæ„ˆè†â€¦â€¦æœˆä¹‹å¤´è„‘ä¸ºæ‚¨å®ˆæŠ¤æ¯ä¸€å¤©çš„å¥åº·ã€‚æœ¬æœˆç‰¹æƒ ï¼šè´è¶æ¢¦ä¸¸ï¼ˆ80æ–‡/ç²’ï¼‰ã€‚",
            "contact": "è¿·é€”ç«¹æ—æ·±å¤„ æ°¸è¿œäº­è¯å±€",
            "icon": "ğŸ’Š",
        },
        {
            "id": "ad_kourindou",
            "title": "é¦™éœ–å ‚ å¤é“å…·åº—",
            "subtitle": "æ£®è¿‘éœ–ä¹‹åŠ© Â· å¤–ç•Œé“å…·ä¸“è¥",
            "description": "æœ¬åº—ç»è¥å„ç±»å¤–ç•Œæµå…¥å“ï¼šGame Boyã€æ‰“ç«æœºã€ä¸æ˜ç”¨é€”çš„å¡‘æ–™æ¿â€¦â€¦è¯†è´§çš„å®¢å®˜è¯·è¿›ã€‚ä¸è®®ä»·ã€‚",
            "contact": "é­”æ³•æ£®æ—å…¥å£å¤„",
            "icon": "ğŸª",
        },
        {
            "id": "ad_moriya",
            "title": "å®ˆçŸ¢ç¥ç¤¾ å¾¡å®ˆç‰¹å–",
            "subtitle": "ä¿¡ä»°å……å€¼ Â· æœ‰æ±‚å¿…åº”",
            "description": "æ–°å¹´é™å®šå¾¡å®ˆä¸Šæ¶ï¼å­¦ä¸šæˆå°±ã€æ‹çˆ±æˆå°±ã€å¼¹å¹•å›é¿â€¦â€¦è¯¹è®¿å­å¤§äººäº²è‡ªåŠ æŒï¼Œä¿¡ä»°å€¼ç¿»å€ã€‚å‚æ‹œå³é€è›™å½¢é¥¼å¹²ã€‚",
            "contact": "å¦–æ€ªå±±å±±é¡¶ å®ˆçŸ¢ç¥ç¤¾",
            "icon": "â›©ï¸",
        },
    ]

    # 5. ç»„è£…å®Œæ•´æ•°æ®
    output = {
        "meta": {
            "title": "å¹»æƒ³ä¹¡æ—¥æŠ¥",
            "title_jp": "å¹»æƒ³éƒ·æ—¥å ±",
            "subtitle": "Gensokyo Daily",
            "edition": datetime.now(timezone.utc).strftime("ç¬¬%Y%m%dæœŸ"),
            "updated_at": datetime.now(timezone.utc).isoformat(),
            "generated_by": "å°„å‘½ä¸¸æ–‡ & GitHub Actions",
            "version": "1.0.0",
        },
        "categories": news_data,
        "weather": weather,
        "ads": ads,
    }

    # 6. å†™å…¥æ–‡ä»¶
    with open(DATA_FILE, "w", encoding="utf-8") as f:
        json.dump(output, f, ensure_ascii=False, indent=2)

    elapsed = time.time() - start_time
    total_items = sum(cat["count"] for cat in news_data.values())

    print("\n" + "=" * 60)
    print(f"âœ… æŠ“å–å®Œæˆï¼å…± {total_items} æ¡æ–°é—»")
    print(f"â±ï¸  è€—æ—¶ {elapsed:.1f} ç§’")
    print(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜è‡³ {DATA_FILE}")
    print("=" * 60)


if __name__ == "__main__":
    main()
