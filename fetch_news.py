#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¹»æƒ³ä¹¡æ—¥æŠ¥ (Gensokyo Daily) â€” æ–°é—»æŠ“å–è„šæœ¬
ä½¿ç”¨ RSSHub ä½œä¸ºæ•°æ®ä¸­é—´ä»¶ï¼Œèšåˆä¸œæ–¹ Project ç›¸å…³èµ„è®¯ã€‚
"""

import json
import os
import re
import time
import hashlib
from datetime import datetime, timedelta, timezone
from typing import Optional

import feedparser
import requests

# ============================================================
# é…ç½®åŒº â€” ä¿®æ”¹è¿™é‡Œæ¥é€‚é…ä½ è‡ªå·±çš„ RSSHub å®ä¾‹
# ============================================================
RSSHUB_BASE = os.environ.get("RSSHUB_BASE", "https://rsshub.app")

# æ•°æ®æ–‡ä»¶è·¯å¾„
DATA_FILE = os.path.join(os.path.dirname(os.path.abspath(__file__)), "news_data.json")

# æ»šåŠ¨æ›´æ–°ç­–ç•¥ï¼šæ¯ä¸ªåˆ†ç±»æœ€å¤šä¿ç•™çš„æ¡ç›®æ•°
MAX_ITEMS_PER_CATEGORY = 50

# æ•°æ®ä¿ç•™å¤©æ•°
MAX_AGE_DAYS = 30

# è¯·æ±‚è¶…æ—¶ï¼ˆç§’ï¼‰
REQUEST_TIMEOUT = 30

# ä¸œæ–¹ç›¸å…³å…³é”®è¯ï¼ˆç”¨äºè¿‡æ»¤æ— å…³å†…å®¹ï¼‰
TOUHOU_KEYWORDS = [
    "ä¸œæ–¹", "æ±æ–¹", "touhou", "Touhou",
    "çµæ¢¦", "éœŠå¤¢", "é­”ç†æ²™", "marisa",
    "å¹»æƒ³ä¹¡", "å¹»æƒ³éƒ·", "gensokyo",
    "åšä¸½", "åšéº—", "çº¢é­”é¦†", "ç´…é­”é¤¨",
    "ZUN", "ä¸Šæµ·çˆ±ä¸½ä¸", "ä¸Šæµ·ã‚¢ãƒªã‚¹",
    "ä¾‹å¤§ç¥­", "çº¢æ¥¼æ¢¦", "ç´…æ¥¼å¤¢",
    "å’²å¤œ", "çªéœ²è¯º", "ãƒãƒ«ãƒ", "cirno",
    "å¦–æ¢¦", "å¦–å¤¢", "å¹½å¹½å­",
    "æ°¸ç³", "è¾‰å¤œ", "è¼å¤œ", "è•¾ç±³è‰äºš",
    "èŠ™å…°æœµéœ²", "å¸•ç§‹è‰", "å°„å‘½ä¸¸æ–‡",
    "æ²³åŸè·å–", "å…«äº‘ç´«", "å…«é›²ç´«",
    "è—¤åŸå¦¹çº¢", "é¬¼äººæ­£é‚ª", "å¤æ˜åœ°è§‰",
    "é£è§å¹½é¦™", "å››å­£æ˜ å§¬", "å°é‡å¡šå°ç”º",
    "å› å¹¡å¸", "é“ƒä»™", "éˆ´ä»™",
    "ä¸œæ–¹çº¢é­”ä¹¡", "ä¸œæ–¹å¦–å¦–æ¢¦", "ä¸œæ–¹æ°¸å¤œæŠ„",
    "ä¸œæ–¹é£ç¥å½•", "ä¸œæ–¹åœ°çµæ®¿", "ä¸œæ–¹æ˜Ÿè²èˆ¹",
    "ä¸œæ–¹ç¥çµåº™", "ä¸œæ–¹è¾‰é’ˆåŸ", "ä¸œæ–¹ç»€ç ä¼ ",
    "ä¸œæ–¹å¤©ç©ºç’‹", "ä¸œæ–¹é¬¼å½¢å…½", "ä¸œæ–¹è™¹é¾™æ´",
    "ä¸œæ–¹å…½ç‹å›­", "ä¸œæ–¹çŒ®åæŠ„", "ä¸œæ–¹åˆšæ¬²å¼‚é—»",
    "thwiki", "THBWiki",
]

# ============================================================
# RSS æºé…ç½®
# ============================================================
RSS_SOURCES = {
    # === å¤´ç‰ˆå¤´æ¡ (Official / ZUN) ===
    "official": {
        "label": "å¤´ç‰ˆå¤´æ¡",
        "feeds": [
            {
                "name": "ä¸œæ–¹å®˜æ–¹èµ„è®¯ç«™",
                "url": f"{RSSHUB_BASE}/touhou-project/news",
                "icon": "ğŸ“°",
                "priority": 1,
            },
            {
                "name": "ZUN æ¨ç‰¹",
                "url": f"{RSSHUB_BASE}/twitter/user/korindo",
                "icon": "ğŸº",
                "priority": 1,
            },
            # Steam æºå·²ç§»é™¤ï¼šSteam è¿”å›å¤§é‡éä¸œæ–¹ç›¸å…³çš„æŠ˜æ‰£/æ’è¡Œå™ªéŸ³ï¼Œ
            # æ›´å¯é çš„åšæ³•æ˜¯ä½¿ç”¨ç‰¹å®š app çš„ news è·¯ç”±æˆ–ä»…ä¾èµ–ç¤¾åŒºæºï¼ˆBç«™/Pixiv/Redditï¼‰ã€‚
        ],
    },
    # === ç¤¾ä¼š/æ°‘ç”Ÿ (Community / Bilibili) ===
    "community": {
        "label": "ç¤¾ä¼šÂ·æ°‘ç”Ÿ",
        "feeds": [
            {
                "name": "Bç«™ä¸œæ–¹çƒ­é—¨è§†é¢‘",
                "url": f"{RSSHUB_BASE}/bilibili/ranking/0/3/1",
                "icon": "ğŸ“º",
                "priority": 1,
                "needs_filter": True,
            },
            {
                "name": "Bç«™ä¸œæ–¹Projecté¢‘é“",
                "url": f"{RSSHUB_BASE}/bilibili/search/hot/ä¸œæ–¹Project",
                "icon": "ğŸ“º",
                "priority": 1,
            },
            {
                "name": "Reddit r/touhou",
                "url": f"{RSSHUB_BASE}/reddit/hot/touhou",
                "icon": "ğŸ’¬",
                "priority": 2,
            },
        ],
    },
    # === è‰ºæœ¯/å‰¯åˆŠ (Art & Culture / Pixiv) ===
    "art": {
        "label": "è‰ºæœ¯Â·å‰¯åˆŠ",
        "feeds": [
            {
                "name": "Pixiv ä¸œæ–¹æ—¥æ¦œ",
                "url": f"{RSSHUB_BASE}/pixiv/ranking/day",
                "icon": "ğŸ¨",
                "priority": 1,
                "needs_filter": True,
            },
            {
                "name": "NicoNico ä¸œæ–¹æ ‡ç­¾",
                "url": f"{RSSHUB_BASE}/nicovideo/tag/æ±æ–¹",
                "icon": "ğŸµ",
                "priority": 2,
            },
        ],
    },
}


# ============================================================
# å·¥å…·å‡½æ•°
# ============================================================


def generate_id(title: str, link: str) -> str:
    """æ ¹æ®æ ‡é¢˜å’Œé“¾æ¥ç”Ÿæˆå”¯ä¸€ ID"""
    raw = f"{title}|{link}"
    return hashlib.md5(raw.encode("utf-8")).hexdigest()[:12]


def is_touhou_related(text: str) -> bool:
    """åˆ¤æ–­æ–‡æœ¬æ˜¯å¦ä¸ä¸œæ–¹ç›¸å…³"""
    if not text:
        return False
    text_lower = text.lower()
    return any(kw.lower() in text_lower for kw in TOUHOU_KEYWORDS)


def clean_html(raw_html: str) -> str:
    """ç§»é™¤ HTML æ ‡ç­¾ï¼Œä¿ç•™çº¯æ–‡æœ¬"""
    if not raw_html:
        return ""
    clean = re.sub(r"<[^>]+>", "", raw_html)
    clean = re.sub(r"\s+", " ", clean).strip()
    return clean[:300]  # æ‘˜è¦æˆªæ–­


def extract_image(entry) -> Optional[str]:
    """ä» RSS æ¡ç›®ä¸­å°½åŠ›æå–ä¸€å¼ å›¾ç‰‡ URL"""
    # å°è¯• media:content
    if hasattr(entry, "media_content") and entry.media_content:
        for media in entry.media_content:
            if "image" in media.get("type", "") or media.get("url", "").endswith(
                (".jpg", ".png", ".webp", ".gif")
            ):
                return media["url"]

    # å°è¯• media:thumbnail
    if hasattr(entry, "media_thumbnail") and entry.media_thumbnail:
        return entry.media_thumbnail[0].get("url")

    # å°è¯• enclosure
    if hasattr(entry, "enclosures") and entry.enclosures:
        for enc in entry.enclosures:
            if "image" in enc.get("type", ""):
                return enc.get("href") or enc.get("url")

    # å°è¯•ä» description/content ä¸­æå– <img>
    content = ""
    if hasattr(entry, "content") and entry.content:
        content = entry.content[0].get("value", "")
    elif hasattr(entry, "summary"):
        content = entry.summary or ""

    img_match = re.search(r'<img[^>]+src=["\']([^"\']+)["\']', content)
    if img_match:
        return img_match.group(1)

    return None


def parse_date(entry) -> str:
    """è§£æå‘å¸ƒæ—¶é—´ï¼Œè¿”å› ISO æ ¼å¼å­—ç¬¦ä¸²"""
    if hasattr(entry, "published_parsed") and entry.published_parsed:
        return datetime(*entry.published_parsed[:6], tzinfo=timezone.utc).isoformat()
    if hasattr(entry, "updated_parsed") and entry.updated_parsed:
        return datetime(*entry.updated_parsed[:6], tzinfo=timezone.utc).isoformat()
    return datetime.now(timezone.utc).isoformat()


def fetch_feed(url: str, timeout: int = REQUEST_TIMEOUT) -> Optional[feedparser.FeedParserDict]:
    """è·å–å¹¶è§£æ RSS feed"""
    try:
        headers = {
            "User-Agent": "Gensokyo-Daily/1.0 (RSS Reader; +https://github.com/gensokyo-daily)"
        }
        resp = requests.get(url, headers=headers, timeout=timeout)
        resp.raise_for_status()
        return feedparser.parse(resp.text)
    except requests.exceptions.RequestException as e:
        print(f"  âš  è·å–å¤±è´¥: {url} â€” {e}")
        return None
    except Exception as e:
        print(f"  âš  è§£æå¤±è´¥: {url} â€” {e}")
        return None


# ============================================================
# å¤©æ°”æ¨¡å—ï¼ˆè™šæ„ - å¹»æƒ³ä¹¡å¤©æ°”ï¼‰
# ============================================================


def generate_gensokyo_weather() -> dict:
    """ç”Ÿæˆå¹»æƒ³ä¹¡å„åœ°çš„è™šæ„å¤©æ°”"""
    import random

    locations = [
        {"name": "åšä¸½ç¥ç¤¾", "name_jp": "åšéº—ç¥ç¤¾"},
        {"name": "äººé—´ä¹‹é‡Œ", "name_jp": "äººé–“ã®é‡Œ"},
        {"name": "çº¢é­”é¦†", "name_jp": "ç´…é­”é¤¨"},
        {"name": "ç™½ç‰æ¥¼", "name_jp": "ç™½ç‰æ¥¼"},
        {"name": "æ°¸è¿œäº­", "name_jp": "æ°¸é äº­"},
        {"name": "å®ˆçŸ¢ç¥ç¤¾", "name_jp": "å®ˆçŸ¢ç¥ç¤¾"},
        {"name": "åœ°çµæ®¿", "name_jp": "åœ°éœŠæ®¿"},
        {"name": "å‘½ï¿½çš„ç¥æ®¿", "name_jp": "å‘½è“®å¯º"},
    ]

    conditions = [
        {"text": "æ™´", "icon": "â˜€ï¸"},
        {"text": "å¤šäº‘", "icon": "â›…"},
        {"text": "é˜´", "icon": "â˜ï¸"},
        {"text": "å°é›¨", "icon": "ğŸŒ¦ï¸"},
        {"text": "é›·é˜µé›¨", "icon": "â›ˆï¸"},
        {"text": "å¼¹å¹•æš´é£", "icon": "ğŸŒ€"},
        {"text": "å¦–é›¾", "icon": "ğŸŒ«ï¸"},
        {"text": "èŠ±ç²‰", "icon": "ğŸŒ¸"},
        {"text": "å¼‚å˜ä¸­", "icon": "âš¡"},
        {"text": "çº¢é›¾", "icon": "ğŸŒ…"},
        {"text": "é›ª", "icon": "â„ï¸"},
        {"text": "æ¨±å¹é›ª", "icon": "ğŸŒ¸"},
    ]

    weather_data = []
    for loc in locations:
        cond = random.choice(conditions)
        temp = random.randint(-5, 35)
        weather_data.append(
            {
                "location": loc["name"],
                "location_jp": loc["name_jp"],
                "condition": cond["text"],
                "icon": cond["icon"],
                "temperature": temp,
            }
        )

    return {
        "updated": datetime.now(timezone.utc).isoformat(),
        "forecasts": weather_data,
    }


# ============================================================
# ä¸»æŠ“å–é€»è¾‘
# ============================================================


def fetch_all_news() -> dict:
    """æŠ“å–æ‰€æœ‰åˆ†ç±»çš„æ–°é—»"""
    print("=" * 60)
    print("ğŸ—ï¸  å¹»æƒ³ä¹¡æ—¥æŠ¥ â€” å¼€å§‹æŠ“å–æ–°é—»")
    print(f"ğŸ“…  {datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')}")
    print("=" * 60)

    all_news = {}
    cutoff_date = datetime.now(timezone.utc) - timedelta(days=MAX_AGE_DAYS)

    for category_key, category_config in RSS_SOURCES.items():
        print(f"\nğŸ“‚ åˆ†ç±»: {category_config['label']}")
        items = []

        for feed_config in category_config["feeds"]:
            print(f"  ğŸ”— æ­£åœ¨è·å–: {feed_config['name']}")
            feed = fetch_feed(feed_config["url"])

            if not feed or not feed.entries:
                print(f"  âš  æ— æ•°æ®æˆ–è·å–å¤±è´¥")
                continue

            count = 0
            for entry in feed.entries:
                title = entry.get("title", "").strip()
                link = entry.get("link", "").strip()
                if not title or not link:
                    continue

                # éœ€è¦è¿‡æ»¤çš„æºï¼šæ£€æŸ¥æ˜¯å¦ä¸ä¸œæ–¹ç›¸å…³
                if feed_config.get("needs_filter"):
                    summary_text = clean_html(
                        entry.get("summary", "") + " " + title
                    )
                    if not is_touhou_related(summary_text):
                        continue

                item = {
                    "id": generate_id(title, link),
                    "title": title,
                    "link": link,
                    "summary": clean_html(entry.get("summary", "")),
                    "image": extract_image(entry),
                    "source": feed_config["name"],
                    "source_icon": feed_config["icon"],
                    "priority": feed_config["priority"],
                    "published": parse_date(entry),
                    "fetched_at": datetime.now(timezone.utc).isoformat(),
                }

                items.append(item)
                count += 1

            print(f"  âœ… è·å–åˆ° {count} æ¡")

        # å»é‡ï¼ˆæŒ‰ idï¼‰
        seen_ids = set()
        unique_items = []
        for item in items:
            if item["id"] not in seen_ids:
                seen_ids.add(item["id"])
                unique_items.append(item)

        # æŒ‰ä¼˜å…ˆçº§ï¼ˆæ•°å€¼è¶Šå°ä¼˜å…ˆçº§è¶Šé«˜ï¼‰å’Œå‘å¸ƒæ—¶é—´é™åºæ’åº
        # é¦–å…ˆæŠŠå‘å¸ƒæ—¶é—´è§£æä¸ºæ—¶é—´æˆ³ï¼Œç¡®ä¿æ’åºè¡Œä¸ºæ­£ç¡®
        def _ts(item):
            try:
                return datetime.fromisoformat(item.get("published", "")).timestamp()
            except Exception:
                return 0

        # key: (priority asc, published_ts desc)
        unique_items.sort(key=lambda x: (x.get("priority", 99), -_ts(x)))

        # æˆªæ–­åˆ°æœ€å¤§æ¡ç›®æ•°
        unique_items = unique_items[:MAX_ITEMS_PER_CATEGORY]

        all_news[category_key] = {
            "label": category_config["label"],
            "items": unique_items,
            "count": len(unique_items),
        }

        print(f"  ğŸ“Š åˆ†ç±» [{category_config['label']}] å…±æ”¶å½• {len(unique_items)} æ¡")

    return all_news


def merge_with_existing(new_data: dict) -> dict:
    """
    ä¸å·²æœ‰æ•°æ®åˆå¹¶ï¼Œå®ç°å¢é‡æ›´æ–°ã€‚
    æ–°æ•°æ®è¦†ç›–åŒ id çš„æ—§æ•°æ®ï¼ŒåŒæ—¶ä¿ç•™æœªè¿‡æœŸçš„æ—§æ¡ç›®ã€‚
    """
    if not os.path.exists(DATA_FILE):
        return new_data

    try:
        with open(DATA_FILE, "r", encoding="utf-8") as f:
            existing = json.load(f)
    except (json.JSONDecodeError, IOError):
        return new_data

    existing_categories = existing.get("categories", {})
    cutoff = datetime.now(timezone.utc) - timedelta(days=MAX_AGE_DAYS)

    for cat_key, cat_data in new_data.items():
        new_items = {item["id"]: item for item in cat_data["items"]}

        # ä»æ—§æ•°æ®ä¸­ä¿ç•™æœªè¿‡æœŸä¸”ä¸é‡å¤çš„æ¡ç›®
        if cat_key in existing_categories:
            for old_item in existing_categories[cat_key].get("items", []):
                if old_item["id"] not in new_items:
                    try:
                        pub_date = datetime.fromisoformat(old_item["published"])
                        if pub_date.tzinfo is None:
                            pub_date = pub_date.replace(tzinfo=timezone.utc)
                        if pub_date > cutoff:
                            new_items[old_item["id"]] = old_item
                    except (ValueError, KeyError):
                        pass

        merged_list = list(new_items.values())
        merged_list.sort(key=lambda x: x.get("published", ""), reverse=True)
        merged_list = merged_list[:MAX_ITEMS_PER_CATEGORY]

        cat_data["items"] = merged_list
        cat_data["count"] = len(merged_list)

    return new_data


def main():
    """ä¸»å…¥å£"""
    start_time = time.time()

    # 1. æŠ“å–æ–°é—»
    news_data = fetch_all_news()

    # 2. åˆå¹¶æ—§æ•°æ®
    news_data = merge_with_existing(news_data)

    # 3. ç”Ÿæˆå¤©æ°”
    weather = generate_gensokyo_weather()

    # 4. è™šæ„å¹¿å‘Š
    ads = [
        {
            "id": "ad_kappa",
            "title": "æ²³ç«¥é‡å·¥ æœ€æ–°ç§‘æŠ€",
            "subtitle": "å…‰å­¦è¿·å½©ã€ç­‰ç¦»å­ç‚®ã€è‡ªåŠ¨é’“é±¼æœº",
            "description": "æ²³åŸè·å–é¢†è¡”ç ”å‘ï¼å¦–æ€ªå±±æ²³ç«¥å·¥ä¸šè”åˆä½“ï¼Œä¸ºæ‚¨æä¾›æœ€å‰æ²¿çš„å¹»æƒ³ç§‘æŠ€ã€‚æ¥æ–™åŠ å·¥ã€å®šåˆ¶å¼¹å¹•ç³»ç»Ÿï¼Œæ¬¢è¿å’¨è¯¢ã€‚",
            "contact": "å¦–æ€ªå±±ç€‘å¸ƒæ— æ²³ç«¥å·¥åŠ",
            "icon": "ğŸ”§",
        },
        {
            "id": "ad_eientei",
            "title": "æ°¸è¿œäº­ ç‰¹ä¾›è¯å‰‚",
            "subtitle": "å…«æ„æ°¸ç³ç›‘åˆ¶ Â· è“¬è±ä¹‹è¯é™¤å¤–",
            "description": "æ„Ÿå†’çµã€è·Œæ‰“ä¸¸ã€å¼¹å¹•åˆ›ä¼¤é€Ÿæ„ˆè†â€¦â€¦æœˆä¹‹å¤´è„‘ä¸ºæ‚¨å®ˆæŠ¤æ¯ä¸€å¤©çš„å¥åº·ã€‚æœ¬æœˆç‰¹æƒ ï¼šè´è¶æ¢¦ä¸¸ï¼ˆ80æ–‡/ç²’ï¼‰ã€‚",
            "contact": "è¿·é€”ç«¹æ—æ·±å¤„ æ°¸è¿œäº­è¯å±€",
            "icon": "ğŸ’Š",
        },
        {
            "id": "ad_kourindou",
            "title": "é¦™éœ–å ‚ å¤é“å…·åº—",
            "subtitle": "æ£®è¿‘éœ–ä¹‹åŠ© Â· å¤–ç•Œé“å…·ä¸“è¥",
            "description": "æœ¬åº—ç»è¥å„ç±»å¤–ç•Œæµå…¥å“ï¼šGame Boyã€æ‰“ç«æœºã€ä¸æ˜ç”¨é€”çš„å¡‘æ–™æ¿â€¦â€¦è¯†è´§çš„å®¢å®˜è¯·è¿›ã€‚ä¸è®®ä»·ã€‚",
            "contact": "é­”æ³•æ£®æ—å…¥å£å¤„",
            "icon": "ğŸª",
        },
        {
            "id": "ad_moriya",
            "title": "å®ˆçŸ¢ç¥ç¤¾ å¾¡å®ˆç‰¹å–",
            "subtitle": "ä¿¡ä»°å……å€¼ Â· æœ‰æ±‚å¿…åº”",
            "description": "æ–°å¹´é™å®šå¾¡å®ˆä¸Šæ¶ï¼å­¦ä¸šæˆå°±ã€æ‹çˆ±æˆå°±ã€å¼¹å¹•å›é¿â€¦â€¦è¯¹è®¿å­å¤§äººäº²è‡ªåŠ æŒï¼Œä¿¡ä»°å€¼ç¿»å€ã€‚å‚æ‹œå³é€è›™å½¢é¥¼å¹²ã€‚",
            "contact": "å¦–æ€ªå±±å±±é¡¶ å®ˆçŸ¢ç¥ç¤¾",
            "icon": "â›©ï¸",
        },
    ]

    # 5. ç»„è£…å®Œæ•´æ•°æ®
    output = {
        "meta": {
            "title": "å¹»æƒ³ä¹¡æ—¥æŠ¥",
            "title_jp": "å¹»æƒ³éƒ·æ—¥å ±",
            "subtitle": "Gensokyo Daily",
            "edition": datetime.now(timezone.utc).strftime("ç¬¬%Y%m%dæœŸ"),
            "updated_at": datetime.now(timezone.utc).isoformat(),
            "generated_by": "å°„å‘½ä¸¸æ–‡ & GitHub Actions",
            "version": "1.0.0",
        },
        "categories": news_data,
        "weather": weather,
        "ads": ads,
    }

    # 6. å†™å…¥æ–‡ä»¶
    with open(DATA_FILE, "w", encoding="utf-8") as f:
        json.dump(output, f, ensure_ascii=False, indent=2)

    elapsed = time.time() - start_time
    total_items = sum(cat["count"] for cat in news_data.values())

    print("\n" + "=" * 60)
    print(f"âœ… æŠ“å–å®Œæˆï¼å…± {total_items} æ¡æ–°é—»")
    print(f"â±ï¸  è€—æ—¶ {elapsed:.1f} ç§’")
    print(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜è‡³ {DATA_FILE}")
    print("=" * 60)


if __name__ == "__main__":
    main()
