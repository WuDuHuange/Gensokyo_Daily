#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¹»æƒ³ä¹¡æ—¥æŠ¥ (Gensokyo Daily) â€” æ–°é—»æŠ“å–è„šæœ¬
ä½¿ç”¨ RSSHub ä½œä¸ºæ•°æ®ä¸­é—´ä»¶ï¼Œèšåˆä¸œæ–¹ Project ç›¸å…³èµ„è®¯ã€‚
"""

import json
import os
import re
import time
import hashlib
from datetime import datetime, timedelta, timezone
from typing import Optional

import feedparser
import requests

# ============================================================
# é…ç½®åŒº â€” ä¿®æ”¹è¿™é‡Œæ¥é€‚é…ä½ è‡ªå·±çš„ RSSHub å®ä¾‹
# ============================================================
RSSHUB_BASE = os.environ.get("RSSHUB_BASE", "https://rsshub.app")

# æ•°æ®æ–‡ä»¶è·¯å¾„
DATA_FILE = os.path.join(os.path.dirname(os.path.abspath(__file__)), "news_data.json")

# æ»šåŠ¨æ›´æ–°ç­–ç•¥ï¼šæ¯ä¸ªåˆ†ç±»æœ€å¤šä¿ç•™çš„æ¡ç›®æ•°
MAX_ITEMS_PER_CATEGORY = 50

# æ•°æ®ä¿ç•™å¤©æ•°
MAX_AGE_DAYS = 30

# è¯·æ±‚è¶…æ—¶ï¼ˆç§’ï¼‰
REQUEST_TIMEOUT = 30

# ä¸œæ–¹ç›¸å…³å…³é”®è¯ 2.0 Proç‰ˆï¼ˆåˆ†ç±»ç®¡ç† + é»‘åå•æœºåˆ¶ï¼‰
# --- æ ¸å¿ƒå…³é”®è¯ï¼šå‡ºç°ä»»æ„ä¸€ä¸ªå³å¯åˆ¤å®šä¸ºä¸œæ–¹ç›¸å…³ ---
CORE_KEYWORDS = [
    "ä¸œæ–¹project", "æ±æ–¹project", "touhou project", "touhou",
    # æ—¥æ–‡å‡åä¸ç‰‡å‡åï¼Œæå‡æ—¥æ–‡/æ—¥åŒºå¹³å°å‘½ä¸­ç‡
    "ãƒˆã‚¦ãƒ›ã‚¦", "ã¨ã†ã»ã†",
    "å¹»æƒ³ä¹¡", "å¹»æƒ³éƒ·", "gensokyo",
    "åšä¸½ç¥ç¤¾", "åšéº—ç¥ç¤¾", "hakurei",
    "ZUN", "ä¸Šæµ·çˆ±ä¸½ä¸", "ä¸Šæµ·ã‚¢ãƒªã‚¹å¹»æ¨‚å›£",
    "ä¾‹å¤§ç¥­", "reitaisai",
    "thwiki", "THBWiki", "ä¸œæ–¹å§",
]

# --- è§’è‰²å…³é”®è¯ ---
CHARACTER_KEYWORDS = [
    "çµæ¢¦", "éœŠå¤¢", "reimu",
    "é­”ç†æ²™", "marisa",
    "å’²å¤œ", "sakuya",
    "çªéœ²è¯º", "ãƒãƒ«ãƒ", "cirno",
    "å¦–æ¢¦", "å¦–å¤¢", "youmu",
    "å¹½å¹½å­", "yuyuko",
    "è•¾ç±³è‰äºš", "remilia",
    "èŠ™å…°æœµéœ²", "flandre",
    "å¸•ç§‹è‰", "patchouli",
    "å°„å‘½ä¸¸æ–‡", "aya shameimaru",
    "æ²³åŸè·å–", "nitori",
    "å…«äº‘ç´«", "å…«é›²ç´«", "yukari",
    "è—¤åŸå¦¹çº¢", "mokou",
    "é¬¼äººæ­£é‚ª", "seija",
    "å¤æ˜åœ°è§‰", "å¤æ˜åœ°æ‹", "satori", "koishi",
    "é£è§å¹½é¦™", "yuuka",
    "å››å­£æ˜ å§¬", "eiki",
    "å°é‡å¡šå°ç”º", "komachi",
    "å› å¹¡å¸", "tewi",
    "é“ƒä»™", "éˆ´ä»™", "reisen",
    "æ°¸ç³", "eirin",
    "è¾‰å¤œ", "è¼å¤œ", "kaguya",
    "çº¢ç¾é“ƒ", "meiling",
    "çˆ±ä¸½ä¸", "alice margatroid",
    "è¥¿è¡Œå¯º", "saigyouji",
    "åšä¸½", "åšéº—",
]

# --- ä½œå“å…³é”®è¯ ---
GAME_KEYWORDS = [
    "çº¢é­”ä¹¡", "ç´…é­”éƒ·", "çº¢é­”é¦†", "ç´…é­”é¤¨",
    "å¦–å¦–æ¢¦", "å¦–ã€…å¤¢",
    "æ°¸å¤œæŠ„",
    "èŠ±æ˜ å¡š",
    "é£ç¥å½•", "é¢¨ç¥éŒ²",
    "åœ°çµæ®¿", "åœ°éœŠæ®¿",
    "æ˜Ÿè²èˆ¹", "æ˜Ÿè“®èˆ¹",
    "ç¥çµåº™", "ç¥éœŠå»Ÿ",
    "è¾‰é’ˆåŸ", "è¼é‡åŸ",
    "ç»€ç ä¼ ", "ç´ºç ä¼",
    "å¤©ç©ºç’‹",
    "é¬¼å½¢å…½", "é¬¼å½¢ç£",
    "è™¹é¾™æ´", "è™¹é¾æ´",
    "å…½ç‹å›­", "ç£ç‹åœ’",
    "çŒ®åæŠ„",
    "åˆšæ¬²å¼‚é—»",
    "ä¸œæ–¹çº¢é­”ä¹¡", "ä¸œæ–¹å¦–å¦–æ¢¦", "ä¸œæ–¹æ°¸å¤œæŠ„",
    "ä¸œæ–¹é£ç¥å½•", "ä¸œæ–¹åœ°çµæ®¿", "ä¸œæ–¹æ˜Ÿè²èˆ¹",
    "ä¸œæ–¹ç¥çµåº™", "ä¸œæ–¹è¾‰é’ˆåŸ", "ä¸œæ–¹ç»€ç ä¼ ",
    "ä¸œæ–¹å¤©ç©ºç’‹", "ä¸œæ–¹é¬¼å½¢å…½", "ä¸œæ–¹è™¹é¾™æ´",
    "ä¸œæ–¹å…½ç‹å›­", "ä¸œæ–¹çŒ®åæŠ„", "ä¸œæ–¹åˆšæ¬²å¼‚é—»",
]

# --- éŸ³ä¹/äºŒåˆ›å…³é”®è¯ ---
MUSIC_KEYWORDS = [
    "ä¸œæ–¹arrange", "ä¸œæ–¹ç¼–æ›²", "ä¸œæ–¹åŒäººéŸ³ä¹",
    "U.N.ã‚ªãƒ¼ã‚¨ãƒ³ã¯å½¼å¥³ãªã®ã‹", "ãƒã‚¯ãƒ­ãƒ•ã‚¡ãƒ³ã‚¿ã‚¸ã‚¢",
    "bad apple", "è‰²ã¯åŒ‚ã¸ã©æ•£ã‚Šã¬ã‚‹ã‚’",
    "ä¸œæ–¹vocal", "ä¸œæ–¹remix",
    "ç§˜å°ä¿±ä¹éƒ¨", "ç§˜å°å€¶æ¥½éƒ¨",
]

# --- é»‘åå•å…³é”®è¯ï¼šå¦‚æœæ ‡é¢˜ä¸­å‡ºç°è¿™äº›è¯ä¸”ä¸åŒ…å«æ ¸å¿ƒå…³é”®è¯ï¼Œåˆ™æ’é™¤ ---
BLACKLIST_KEYWORDS = [
    "ä¸œæ–¹å«è§†", "ä¸œæ–¹è´¢å¯Œ", "ä¸œæ–¹èˆªç©º", "ä¸œæ–¹æ˜ç ",
    "ä¸œæ–¹é›¨è™¹", "ä¸œæ–¹ç”µæ°”", "ä¸œæ–¹è¯åˆ¸", "ä¸œæ–¹é€šä¿¡",
    "ä¸œæ–¹å›­æ—", "ä¸œæ–¹æ—¥å‡", "ä¸œæ–¹ç››è™¹", "ä¸œæ–¹é“å¡”",
    "ä¸œæ–¹ç¾é£Ÿ", "ä¸œæ–¹ç”„é€‰", "ä¸œæ–¹ä¸è´¥",
    "orient", "oriental securities",
]

# åˆå¹¶ä¸ºæ€»å…³é”®è¯åˆ—è¡¨
TOUHOU_KEYWORDS = CORE_KEYWORDS + CHARACTER_KEYWORDS + GAME_KEYWORDS + MUSIC_KEYWORDS

# ============================================================
# RSS æºé…ç½®
# ============================================================
RSS_SOURCES = {
    # === å¤´ç‰ˆå¤´æ¡ (Official) ===
    "official": {
        "label": "å¤´ç‰ˆå¤´æ¡",
        "feeds": [
            {
                "name": "ä¸œæ–¹å®˜æ–¹èµ„è®¯ç«™",
                # ä¼˜å…ˆä½¿ç”¨åŸç”Ÿ WordPress feedï¼Œç»•è¿‡ RSSHub
                "url": "https://touhou-project.news/feed.rss",
                "icon": "ğŸ“°",
                "priority": 1,
            },

        ],
    },

    # === ç¤¾ä¼š/æ°‘ç”Ÿ (Community) ===
    "community": {
        "label": "ç¤¾ä¼šÂ·æ°‘ç”Ÿ",
        "feeds": [
            {
                "name": "Bç«™ MMDæ¦œ", 
                "url": f"{RSSHUB_BASE}/bilibili/ranking/25/3/1", # 25=MMDåˆ†åŒº, 3=ä¸‰æ—¥æ¦œ
                "icon": "ğŸ’ƒ",
                "priority": 1,
                "needs_filter": True, # ä¾ç„¶éœ€è¦å…³é”®è¯è¿‡æ»¤ï¼Œé˜²æ­¢æŠ“åˆ°åŸç¥/å´©é“
            },
            {
                "name": "Bç«™ MADæ¦œ", 
                "url": f"{RSSHUB_BASE}/bilibili/ranking/24/3/1", # 24=MADåˆ†åŒº
                "icon": "ğŸ¬",
                "priority": 1,
                "needs_filter": True,
            },
            {
                "name": "Bç«™ æ¸¸æˆæ¦œ", 
                "url": f"{RSSHUB_BASE}/bilibili/ranking/17/3/1", # 17=å•æœºåˆ†åŒº
                "icon": "ğŸ®",
                "priority": 2,
                "needs_filter": True,
            },
            {
                "name": "Reddit r/touhou",
                # ç›´æ¥ä½¿ç”¨ Reddit åŸç”Ÿ RSS
                "url": "https://www.reddit.com/r/touhou/new/.rss",
                "icon": "ğŸ’¬",
                "priority": 2,
            },
            {
                "name": "THWiki æœ€è¿‘æ›´æ”¹",
                # ä½¿ç”¨ THWiki åŸç”Ÿ Atom feed
                "url": "https://thwiki.cc/index.php?title=Special:%E6%9C%80%E8%BF%91%E6%9B%B4%E6%94%B9&feed=atom",
                "icon": "ğŸ“š",
                "priority": 3,
            },
        ],
    },

    # === è‰ºæœ¯/å‰¯åˆŠ (Art) ===
    "art": {
        "label": "è‰ºæœ¯Â·å‰¯åˆŠ",
        "feeds": [
            {
                "name": "Safebooru (Touhou)",
                # ä½¿ç”¨å‹å¥½çš„ Booru ç«™ç‚¹æ›¿ä»£ Pixiv
                "url": "https://safebooru.org/index.php?page=rss&s=post&q=touhou",
                "icon": "ğŸ¨",
                "priority": 1,
            },
        ],
    },
}


# ============================================================
# å·¥å…·å‡½æ•°
# ============================================================


def generate_id(title: str, link: str) -> str:
    """æ ¹æ®æ ‡é¢˜å’Œé“¾æ¥ç”Ÿæˆå”¯ä¸€ ID"""
    raw = f"{title}|{link}"
    return hashlib.md5(raw.encode("utf-8")).hexdigest()[:12]


def is_touhou_related(text: str) -> bool:
    """æ›´æ™ºèƒ½çš„ä¸œæ–¹ç›¸å…³åˆ¤æ–­é€»è¾‘ï¼ˆ2.0 Proç‰ˆï¼‰"""
    if not text:
        return False
    text_lower = text.lower()

    # 1. å…ˆæ£€æŸ¥é»‘åå•ï¼ˆä¸€ç¥¨å¦å†³ï¼‰
    for bad_word in BLACKLIST_KEYWORDS:
        if bad_word.lower() in text_lower:
            # ç‰¹æ®Šæƒ…å†µï¼šå¦‚æœåŒæ—¶å‡ºç°äº†"ä¸œæ–¹Project"æˆ–"ZUN"ï¼Œåˆ™æ— è§†é»‘åå•
            # ï¼ˆé˜²æ­¢"ä¸œæ–¹å«è§†æŠ¥é“äº†ä¸œæ–¹Projectå±•ä¼š"è¿™ç§æ–°é—»è¢«è¯¯æ€ï¼‰
            if "project" in text_lower or "zun" in text_lower:
                continue
            return False

    # 2. å¼ºåŒ¹é…ï¼šä»»ä¸€æ ¸å¿ƒ/è§’è‰²/ä½œå“/éŸ³ä¹å…³é”®è¯å‡ºç°å³åˆ¤å®šä¸ºä¸œæ–¹ç›¸å…³
    positive_lists = CORE_KEYWORDS + CHARACTER_KEYWORDS + GAME_KEYWORDS + MUSIC_KEYWORDS
    for kw in positive_lists:
        if kw.lower() in text_lower:
            return True

    # 3. å¼±åŒ¹é…å¤„ç†ï¼šå•ç‹¬çš„â€œä¸œæ–¹/æ±æ–¹â€æ˜¯é«˜åº¦æ­§ä¹‰çš„è¯ï¼Œ
    #    ä»…åœ¨åŒæ—¶å‡ºç°è§’è‰²/ä½œå“/éŸ³ä¹ç­‰å…·ä½“å…³é”®è¯æ—¶æ‰åˆ¤å®šä¸ºä¸œæ–¹ç›¸å…³ã€‚
    if "ä¸œæ–¹" in text or "æ±æ–¹" in text_lower:
        for kw in CHARACTER_KEYWORDS + GAME_KEYWORDS + MUSIC_KEYWORDS:
            if kw.lower() in text_lower:
                return True
        return False

    # æœªå‘½ä¸­ä»»ä½•åˆ¤å®šæ¡ä»¶ -> éä¸œæ–¹ç›¸å…³
    return False


def is_important_zun_tweet(text: str) -> bool:
    """åˆ¤æ–­ ZUN çš„æ¨ç‰¹æ˜¯å¦åŒ…å«é‡è¦ä¿¡æ¯ï¼ˆç”¨äº is_zun æ ‡è®°æºï¼‰ã€‚

    ç­–ç•¥ï¼šåŸºäºå…³é”®è¯åŠ æƒï¼ŒåŒ…å«å‘å¸ƒ/å¼€å‘/ä¾‹å¤§ç¥­/å…¬å¼€ç­‰è¯è§†ä¸ºé‡è¦ï¼›
    åŒæ—¶å¦‚æœå¸¦å›¾ç‰‡ä¹Ÿå¯è§†ä½œè¾ƒé‡è¦çš„åŠ¨æ€ã€‚
    """
    if not text:
        return False
    text_lower = text.lower()

    keywords = [
        "æ–°ä½œ", "ä½“é¨“ç‰ˆ", "ä½“é¨“", "å®Œæˆ", "å…¥ç¨¿", "ç™¼å”®", "å…¬é–‹", "å‘å¸ƒ", "ç™ºå£²", "ç™ºè¡¨", "å‘ŠçŸ¥", "ãƒªãƒªãƒ¼ã‚¹",
        "ä¾‹å¤§ç¥­", "ã‚³ãƒŸã‚±", "å¤ã‚³ãƒŸ", "å†¬ã‚³ãƒŸ", "reitaisai",
        "release", "steam", "é…ä¿¡", "å…¬é–‹", "interview", "ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼",
        # æ—¥æ–‡å‡å/ç‰‡å‡åä¸è‹±æ–‡
        "ãƒˆã‚¦ãƒ›ã‚¦", "ã¨ã†ã»ã†", "touhou", "æ±æ–¹", "touhou project", "æ±æ–¹project",
    ]

    for kw in keywords:
        if kw.lower() in text_lower:
            return True

    # å¦‚æœåŒ…å«å›¾ç‰‡æ ‡ç­¾ï¼Œé€šå¸¸ä¹Ÿæ¯”è¾ƒå€¼å¾—å…³æ³¨
    if "<img" in text_lower:
        return True

    return False


def clean_html(raw_html: str) -> str:
    """ç§»é™¤ HTML æ ‡ç­¾ï¼Œä¿ç•™çº¯æ–‡æœ¬"""
    if not raw_html:
        return ""
    clean = re.sub(r"<[^>]+>", "", raw_html)
    clean = re.sub(r"\s+", " ", clean).strip()
    return clean[:300]  # æ‘˜è¦æˆªæ–­


def extract_image(entry) -> Optional[str]:
    """ä» RSS æ¡ç›®ä¸­å°½åŠ›æå–ä¸€å¼ å›¾ç‰‡ URL"""
    # å°è¯• media:content
    if hasattr(entry, "media_content") and entry.media_content:
        for media in entry.media_content:
            if "image" in media.get("type", "") or media.get("url", "").endswith(
                (".jpg", ".png", ".webp", ".gif")
            ):
                return media["url"]

    # å°è¯• media:thumbnail
    if hasattr(entry, "media_thumbnail") and entry.media_thumbnail:
        return entry.media_thumbnail[0].get("url")

    # å°è¯• enclosure
    if hasattr(entry, "enclosures") and entry.enclosures:
        for enc in entry.enclosures:
            if "image" in enc.get("type", ""):
                return enc.get("href") or enc.get("url")

    # å°è¯•ä» description/content ä¸­æå– <img>
    content = ""
    if hasattr(entry, "content") and entry.content:
        content = entry.content[0].get("value", "")
    elif hasattr(entry, "summary"):
        content = entry.summary or ""

    img_match = re.search(r'<img[^>]+src=["\']([^"\']+)["\']', content)
    if img_match:
        return img_match.group(1)

    return None


def parse_date(entry) -> str:
    """è§£æå‘å¸ƒæ—¶é—´ï¼Œè¿”å› ISO æ ¼å¼å­—ç¬¦ä¸²"""
    if hasattr(entry, "published_parsed") and entry.published_parsed:
        return datetime(*entry.published_parsed[:6], tzinfo=timezone.utc).isoformat()
    if hasattr(entry, "updated_parsed") and entry.updated_parsed:
        return datetime(*entry.updated_parsed[:6], tzinfo=timezone.utc).isoformat()
    return datetime.now(timezone.utc).isoformat()


def fetch_feed(url: str, timeout: int = REQUEST_TIMEOUT) -> Optional[feedparser.FeedParserDict]:
    """è·å–å¹¶è§£æ RSS feed"""
    try:
        headers = {
            "User-Agent": "Gensokyo-Daily/1.0 (RSS Reader; +https://github.com/gensokyo-daily)"
        }
        with requests.Session() as session:
            resp = session.get(url, headers=headers, timeout=timeout)

        # å¦‚æœè¿”å›é 2xxï¼Œå°½é‡æ‰“å°æ›´å¤šä¿¡æ¯ä»¥ä¾¿æ’æŸ¥
        if resp.status_code >= 400:
            snippet = resp.text[:500].replace("\n", " ") if resp.text else ""
            print(f"  âš  è·å–å¤±è´¥: {url} â€” HTTP {resp.status_code} {resp.reason}")
            if snippet:
                print(f"    â†’ å“åº”ç‰‡æ®µ: {snippet}")
            return None

        parsed = feedparser.parse(resp.text)
        # feedparser æœ‰ bozo æ ‡å¿—è¡¨ç¤ºè§£ææ—¶å‡ºç°å¼‚å¸¸
        if getattr(parsed, "bozo", False):
            be = getattr(parsed, "bozo_exception", None)
            print(f"  âš  è§£æè­¦å‘Š: {url} â€” {be}")

        return parsed
    except requests.exceptions.RequestException as e:
        # requests å¼‚å¸¸æ—¶å°½é‡è¾“å‡ºçŠ¶æ€ä¸å“åº”ç‰‡æ®µï¼ˆå¦‚æœæœ‰ï¼‰
        msg = str(e)
        resp = getattr(e, "response", None)
        if resp is not None:
            try:
                snippet = resp.text[:500].replace("\n", " ")
            except Exception:
                snippet = "(unable to read response body)"
            print(f"  âš  è·å–å¤±è´¥: {url} â€” HTTP {resp.status_code} {resp.reason} â€” {msg}")
            print(f"    â†’ å“åº”ç‰‡æ®µ: {snippet}")
        else:
            print(f"  âš  è·å–å¤±è´¥: {url} â€” {msg}")
        return None
    except Exception as e:
        print(f"  âš  è§£æå¤±è´¥: {url} â€” {e}")
        return None


# ============================================================
# å¤©æ°”æ¨¡å—ï¼ˆè™šæ„ - å¹»æƒ³ä¹¡å¤©æ°”ï¼‰
# ============================================================


def generate_gensokyo_weather() -> dict:
    """ç”Ÿæˆå¹»æƒ³ä¹¡å„åœ°çš„è™šæ„å¤©æ°”"""
    import random

    locations = [
        {"name": "åšä¸½ç¥ç¤¾", "name_jp": "åšéº—ç¥ç¤¾"},
        {"name": "äººé—´ä¹‹é‡Œ", "name_jp": "äººé–“ã®é‡Œ"},
        {"name": "çº¢é­”é¦†", "name_jp": "ç´…é­”é¤¨"},
        {"name": "ç™½ç‰æ¥¼", "name_jp": "ç™½ç‰æ¥¼"},
        {"name": "æ°¸è¿œäº­", "name_jp": "æ°¸é äº­"},
        {"name": "å®ˆçŸ¢ç¥ç¤¾", "name_jp": "å®ˆçŸ¢ç¥ç¤¾"},
        {"name": "åœ°çµæ®¿", "name_jp": "åœ°éœŠæ®¿"},
        {"name": "å‘½ï¿½çš„ç¥æ®¿", "name_jp": "å‘½è“®å¯º"},
    ]

    conditions = [
        {"text": "æ™´", "icon": "â˜€ï¸"},
        {"text": "å¤šäº‘", "icon": "â›…"},
        {"text": "é˜´", "icon": "â˜ï¸"},
        {"text": "å°é›¨", "icon": "ğŸŒ¦ï¸"},
        {"text": "é›·é˜µé›¨", "icon": "â›ˆï¸"},
        {"text": "å¼¹å¹•æš´é£", "icon": "ğŸŒ€"},
        {"text": "å¦–é›¾", "icon": "ğŸŒ«ï¸"},
        {"text": "èŠ±ç²‰", "icon": "ğŸŒ¸"},
        {"text": "å¼‚å˜ä¸­", "icon": "âš¡"},
        {"text": "çº¢é›¾", "icon": "ğŸŒ…"},
        {"text": "é›ª", "icon": "â„ï¸"},
        {"text": "æ¨±å¹é›ª", "icon": "ğŸŒ¸"},
    ]

    weather_data = []
    for loc in locations:
        cond = random.choice(conditions)
        temp = random.randint(-5, 35)
        weather_data.append(
            {
                "location": loc["name"],
                "location_jp": loc["name_jp"],
                "condition": cond["text"],
                "icon": cond["icon"],
                "temperature": temp,
            }
        )

    return {
        "updated": datetime.now(timezone.utc).isoformat(),
        "forecasts": weather_data,
    }


# ============================================================
# ä¸»æŠ“å–é€»è¾‘
# ============================================================


def fetch_all_news() -> dict:
    """æŠ“å–æ‰€æœ‰åˆ†ç±»çš„æ–°é—»"""
    print("=" * 60)
    print("ğŸ—ï¸  å¹»æƒ³ä¹¡æ—¥æŠ¥ â€” å¼€å§‹æŠ“å–æ–°é—»")
    print(f"ğŸ”— ä½¿ç”¨ RSSHUB_BASE: {RSSHUB_BASE}")
    print(f"ğŸ“…  {datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')}")
    print("=" * 60)

    all_news = {}
    cutoff_date = datetime.now(timezone.utc) - timedelta(days=MAX_AGE_DAYS)

    for category_key, category_config in RSS_SOURCES.items():
        print(f"\nğŸ“‚ åˆ†ç±»: {category_config['label']}")
        items = []

        for feed_config in category_config["feeds"]:
            print(f"  ğŸ”— æ­£åœ¨è·å–: {feed_config['name']}")
            feed = fetch_feed(feed_config["url"])

            if not feed or not feed.entries:
                print(f"  âš  æ— æ•°æ®æˆ–è·å–å¤±è´¥")
                continue

            count = 0
            for entry in feed.entries:
                title = entry.get("title", "").strip()
                link = entry.get("link", "").strip()
                if not title or not link:
                    continue

                # éœ€è¦è¿‡æ»¤çš„æºï¼šæ£€æŸ¥æ˜¯å¦ä¸ä¸œæ–¹ç›¸å…³
                if feed_config.get("needs_filter"):
                    summary_text = clean_html(
                        entry.get("summary", "") + " " + title
                    )
                    if not is_touhou_related(summary_text):
                        continue

                # ZUN ä¸“å±è¿‡æ»¤ï¼šå¯¹æ ‡è®°ä¸º is_zun çš„æºåšé‡è¦æ€§åˆ¤æ–­
                if feed_config.get("is_zun"):
                    full_text = clean_html(entry.get("summary", "") + " " + title)
                    if not is_important_zun_tweet(full_text):
                        continue

                item = {
                    "id": generate_id(title, link),
                    "title": title,
                    "link": link,
                    "summary": clean_html(entry.get("summary", "")),
                    "image": extract_image(entry),
                    "source": feed_config["name"],
                    "source_icon": feed_config["icon"],
                    "priority": feed_config["priority"],
                    "published": parse_date(entry),
                    "fetched_at": datetime.now(timezone.utc).isoformat(),
                }

                items.append(item)
                count += 1

            print(f"  âœ… è·å–åˆ° {count} æ¡")

        # å»é‡ï¼ˆæŒ‰ idï¼‰
        seen_ids = set()
        unique_items = []
        for item in items:
            if item["id"] not in seen_ids:
                seen_ids.add(item["id"])
                unique_items.append(item)

        # æŒ‰ä¼˜å…ˆçº§ï¼ˆæ•°å€¼è¶Šå°ä¼˜å…ˆçº§è¶Šé«˜ï¼‰å’Œå‘å¸ƒæ—¶é—´é™åºæ’åº
        # é¦–å…ˆæŠŠå‘å¸ƒæ—¶é—´è§£æä¸ºæ—¶é—´æˆ³ï¼Œç¡®ä¿æ’åºè¡Œä¸ºæ­£ç¡®
        def _ts(item):
            try:
                return datetime.fromisoformat(item.get("published", "")).timestamp()
            except Exception:
                return 0

        # key: (priority asc, published_ts desc)
        unique_items.sort(key=lambda x: (x.get("priority", 99), -_ts(x)))

        # æˆªæ–­åˆ°æœ€å¤§æ¡ç›®æ•°
        unique_items = unique_items[:MAX_ITEMS_PER_CATEGORY]

        all_news[category_key] = {
            "label": category_config["label"],
            "items": unique_items,
            "count": len(unique_items),
        }

        print(f"  ğŸ“Š åˆ†ç±» [{category_config['label']}] å…±æ”¶å½• {len(unique_items)} æ¡")

    return all_news


def merge_with_existing(new_data: dict) -> dict:
    """
    ä¸å·²æœ‰æ•°æ®åˆå¹¶ï¼Œå®ç°å¢é‡æ›´æ–°ã€‚
    æ–°æ•°æ®è¦†ç›–åŒ id çš„æ—§æ•°æ®ï¼ŒåŒæ—¶ä¿ç•™æœªè¿‡æœŸçš„æ—§æ¡ç›®ã€‚
    """
    if not os.path.exists(DATA_FILE):
        return new_data

    try:
        with open(DATA_FILE, "r", encoding="utf-8") as f:
            existing = json.load(f)
    except (json.JSONDecodeError, IOError):
        return new_data

    existing_categories = existing.get("categories", {})
    cutoff = datetime.now(timezone.utc) - timedelta(days=MAX_AGE_DAYS)

    for cat_key, cat_data in new_data.items():
        new_items = {item["id"]: item for item in cat_data["items"]}

        # ä»æ—§æ•°æ®ä¸­ä¿ç•™æœªè¿‡æœŸä¸”ä¸é‡å¤çš„æ¡ç›®
        if cat_key in existing_categories:
            for old_item in existing_categories[cat_key].get("items", []):
                if old_item["id"] not in new_items:
                    try:
                        pub_date = datetime.fromisoformat(old_item["published"])
                        if pub_date.tzinfo is None:
                            pub_date = pub_date.replace(tzinfo=timezone.utc)
                        if pub_date > cutoff:
                            new_items[old_item["id"]] = old_item
                    except (ValueError, KeyError):
                        pass

        merged_list = list(new_items.values())
        merged_list.sort(key=lambda x: x.get("published", ""), reverse=True)
        merged_list = merged_list[:MAX_ITEMS_PER_CATEGORY]

        cat_data["items"] = merged_list
        cat_data["count"] = len(merged_list)

    return new_data


def main():
    """ä¸»å…¥å£"""
    start_time = time.time()

    # 1. æŠ“å–æ–°é—»
    news_data = fetch_all_news()

    # 2. åˆå¹¶æ—§æ•°æ®
    news_data = merge_with_existing(news_data)

    # 3. ç”Ÿæˆå¤©æ°”
    weather = generate_gensokyo_weather()

    # 4. è™šæ„å¹¿å‘Š
    ads = [
        {
            "id": "ad_kappa",
            "title": "æ²³ç«¥é‡å·¥ æœ€æ–°ç§‘æŠ€",
            "subtitle": "å…‰å­¦è¿·å½©ã€ç­‰ç¦»å­ç‚®ã€è‡ªåŠ¨é’“é±¼æœº",
            "description": "æ²³åŸè·å–é¢†è¡”ç ”å‘ï¼å¦–æ€ªå±±æ²³ç«¥å·¥ä¸šè”åˆä½“ï¼Œä¸ºæ‚¨æä¾›æœ€å‰æ²¿çš„å¹»æƒ³ç§‘æŠ€ã€‚æ¥æ–™åŠ å·¥ã€å®šåˆ¶å¼¹å¹•ç³»ç»Ÿï¼Œæ¬¢è¿å’¨è¯¢ã€‚",
            "contact": "å¦–æ€ªå±±ç€‘å¸ƒæ— æ²³ç«¥å·¥åŠ",
            "icon": "ğŸ”§",
        },
        {
            "id": "ad_eientei",
            "title": "æ°¸è¿œäº­ ç‰¹ä¾›è¯å‰‚",
            "subtitle": "å…«æ„æ°¸ç³ç›‘åˆ¶ Â· è“¬è±ä¹‹è¯é™¤å¤–",
            "description": "æ„Ÿå†’çµã€è·Œæ‰“ä¸¸ã€å¼¹å¹•åˆ›ä¼¤é€Ÿæ„ˆè†â€¦â€¦æœˆä¹‹å¤´è„‘ä¸ºæ‚¨å®ˆæŠ¤æ¯ä¸€å¤©çš„å¥åº·ã€‚æœ¬æœˆç‰¹æƒ ï¼šè´è¶æ¢¦ä¸¸ï¼ˆ80æ–‡/ç²’ï¼‰ã€‚",
            "contact": "è¿·é€”ç«¹æ—æ·±å¤„ æ°¸è¿œäº­è¯å±€",
            "icon": "ğŸ’Š",
        },
        {
            "id": "ad_kourindou",
            "title": "é¦™éœ–å ‚ å¤é“å…·åº—",
            "subtitle": "æ£®è¿‘éœ–ä¹‹åŠ© Â· å¤–ç•Œé“å…·ä¸“è¥",
            "description": "æœ¬åº—ç»è¥å„ç±»å¤–ç•Œæµå…¥å“ï¼šGame Boyã€æ‰“ç«æœºã€ä¸æ˜ç”¨é€”çš„å¡‘æ–™æ¿â€¦â€¦è¯†è´§çš„å®¢å®˜è¯·è¿›ã€‚ä¸è®®ä»·ã€‚",
            "contact": "é­”æ³•æ£®æ—å…¥å£å¤„",
            "icon": "ğŸª",
        },
        {
            "id": "ad_moriya",
            "title": "å®ˆçŸ¢ç¥ç¤¾ å¾¡å®ˆç‰¹å–",
            "subtitle": "ä¿¡ä»°å……å€¼ Â· æœ‰æ±‚å¿…åº”",
            "description": "æ–°å¹´é™å®šå¾¡å®ˆä¸Šæ¶ï¼å­¦ä¸šæˆå°±ã€æ‹çˆ±æˆå°±ã€å¼¹å¹•å›é¿â€¦â€¦è¯¹è®¿å­å¤§äººäº²è‡ªåŠ æŒï¼Œä¿¡ä»°å€¼ç¿»å€ã€‚å‚æ‹œå³é€è›™å½¢é¥¼å¹²ã€‚",
            "contact": "å¦–æ€ªå±±å±±é¡¶ å®ˆçŸ¢ç¥ç¤¾",
            "icon": "â›©ï¸",
        },
    ]

    # 5. ç»„è£…å®Œæ•´æ•°æ®
    output = {
        "meta": {
            "title": "å¹»æƒ³ä¹¡æ—¥æŠ¥",
            "title_jp": "å¹»æƒ³éƒ·æ—¥å ±",
            "subtitle": "Gensokyo Daily",
            "edition": datetime.now(timezone.utc).strftime("ç¬¬%Y%m%dæœŸ"),
            "updated_at": datetime.now(timezone.utc).isoformat(),
            "generated_by": "å°„å‘½ä¸¸æ–‡ & GitHub Actions",
            "version": "1.0.0",
        },
        "categories": news_data,
        "weather": weather,
        "ads": ads,
    }

    # 6. å†™å…¥æ–‡ä»¶
    with open(DATA_FILE, "w", encoding="utf-8") as f:
        json.dump(output, f, ensure_ascii=False, indent=2)

    elapsed = time.time() - start_time
    total_items = sum(cat["count"] for cat in news_data.values())

    print("\n" + "=" * 60)
    print(f"âœ… æŠ“å–å®Œæˆï¼å…± {total_items} æ¡æ–°é—»")
    print(f"â±ï¸  è€—æ—¶ {elapsed:.1f} ç§’")
    print(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜è‡³ {DATA_FILE}")
    print("=" * 60)


if __name__ == "__main__":
    main()
